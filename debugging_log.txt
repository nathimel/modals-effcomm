The purpose of this file is to track bugs that often recur, and document how they were resolved.

6.29.22: incomplete looking plot

The high informativity, low communicative cost languages don't seem to be getting explored after some updates, most recently to altk's optimization and agent modules. The space used to be fully explored. I have seen this problem before too. 
Tried:
    Looking into AddPoint:
    - uncommenting AddPoint() as a mutation to increase informativity, because at some point previously it wasn't helping -- presumably because high informativity space was getting explored fine.
    - looking into how informativity is measured in AddPoint. This doesn't matter so much as long as informativity isn't already recorded as 1.0, because it just searches the language for a point not yet covered, and then adds an expression to cover exactly that point.
    - running optimization with only AddPoint as a mutation, but this doesn't work because apparently you need all mutations in order for there to be a possible mutation when sample_mutate() is called.

    Looking into the history of some of the plots on github:
    - Looks like I had a similar issue when testing out various mutations, and found that RemovePoint was the culprit; apparently removing it as a possible mutation allowed the high informativity space to be explored. But it's possible that this wasn't the only cause, or at least something else may have motivated me to keep RemovePoint, because it still exists in the codebase.
        - Sure enough, commenting it out didn't improve on dev setting or a full run on indicator_literal. 

    - with the pragmatic settings, the effect is less dramatic, but there's still a surprising lack of exploration. I know this because I added a highly complex, but still perfectly informative language (one with many synonyms) and the points in between this and the perfect informative language with no synonyms were not found.


6.29.22: random seed not producing deterministic behavior

I have never seen deterministic behavior yet, but making the entry now. All scripts call the set_seed() function, which updates the only two libraries I know of that I'm using that use randomness:
    - random
    - numpy

7.1.22: pandas datatype confusion seems to not be a problem anymore. 

I'm not particularly worried about removing the logic which prevented floats from being read as strings, because the statistics and data look the same with and without. For reference though, pandas was once confused about there being mixed data types and I had to us pd.to_numeric() to convert all the appropriate columns back to floats.

7.1.22: strange plot with correct data but huge complexity limit at like 3e10

This happened once right after deleting altk's complexity module for refactoring, but then I haven't been able to reproduce again.


7.3.22: Highly complex DLSAV languages

    Looking into specific languages:
    - This has been an issue ever since I added dlsav functionality, and I should have noticed it before, but I am now addressing it as part of the more general debugging process of aligning / comparing the new and old codebase results.
    - There are far more vanderkok / dlsav languages with high complexity (> 35) in the new codebase than in the old one. So either I'm sampling in a very different way, or I'm measuring dlsav incorrectly. So I started inspecting the highest complexity, dlsav languages of the new codebase. Comparing half_credit_literal, I found 7 languages over 35 complexity that were dlsav, and all of them had complexity 36, except for the one highly synonymous language I added as a sanity check. Looking at the first one, `sampled_lang_1881` here were the meanings: 

    >>> for e in lang.expressions:
    ...     print(e.meaning)
    ... 
    ['strong+deontic']
    ['strong+deontic', 'strong+circumstantial'] *
    ['strong+epistemic']
    ['strong+epistemic', 'strong+circumstantial']
    ['weak+circumstantial']
    ['weak+circumstantial', 'strong+circumstantial'] *
    ['weak+deontic']
    ['weak+deontic', 'weak+circumstantial']
    ['weak+epistemic', 'strong+epistemic']
    ['weak+epistemic', 'weak+circumstantial']

    I then used their array representation to check whether the old codebase thinks this is a dlsav language. And sure enough! I found that two expressions were indeed violating the dlsav criterion, but had counted in the new codebase as passing the criterion. To see why, look at the two lists with asterisks * after them: we have two kinds of ambiguity within the root domain: one expression has fixed circumstantial flavor, but is variable-force, whereas another expression has strong force, but variable-flavor across circumstantial and deontic. 

    I ported the numpy array logic from the old codebase instead of working with the more general meaning representation as a list of strings, and 
    then the new and old codebases agreed that this language was not a dlsav language. After running the new code, the plot still looked dlsav languages were too complex -- or at least they weren't isolated to the frontier enough. This time there were only two languages with complexity > 35: `sampled_lang_1873` and `Synonymy`. Here's sampled_lang_1873:

    ['strong+circumstantial']
    ['strong+deontic']
    ['strong+circumstantial', 'strong+deontic']
    ['strong+circumstantial', 'strong+epistemic']
    ['strong+circumstantial', 'strong+deontic', 'strong+epistemic']
    ['weak+circumstantial']
    ['weak+deontic']
    ['weak+circumstantial', 'weak+deontic']
    ['weak+epistemic', 'strong+epistemic']
    ['weak+epistemic', 'weak+deontic'] 

    There are no violations of dlsav with this language. So something may have gone wrong with the logic integrating numpy arrays with the new altk meanings, or sampling is weird. I checked with the old codebase, and sure enough it concurs that this is a vanderklok_ok language. I squinted at the plots again, and realized that the scale on the y-axis was different, which is what makes the difference in complexity look so dramatic. Because this language only has complexity 36, and sure enough the old codebase plot does indeed have a dlsav language of higher than 35 complexity.

    Given this, there are actually some more serious problems, guided by the fact that the old plot has dlsav languages highly clustered along the frontier, _and_ all the very unnnatural (0 degree nauze) langs are at the high comm_cost side of the plot (around 0.8). In contrast, all of the unnatural (degree nauze) languages in the new codebase are at about 0.5 informativity. In the old codebase, 0.5 informativity has medium-naturalness, and the huge stack of dark blue languages don't start until about 0.75 and 0.8: furthermore, it looks like at least half of all the languages sampled / explored are in this high comm_cost region. This region is extremely unexplored in the new codebase. This _could_ be because of the sample size: in the new codebase, the same parameter settings result in 5659 total langs, whereas `67962` in old -- a factor of more than 10. 

        - Ran with a (random, not generation) sample size of 15k, and increased generations from 200 to 400. Resulted in 16912 total langs. The trend looks the same: dark blue stack of langs are at 0.5, rather than 0.8.

        - Okay, here's something worrying / reassuring: I printed out the dataframe of the old codebase to check out these high comm_cost languages. The simplicity measure is negative, which is a big logic error. It may still be that informativity was computed correctly, but it should be checked; after all, the we know the minimum informativity for literal speakers is exactly the prior (typically 0.1667 because I use uniform and a space of (2,3)).

        - Here's the dataframe, sorted by comm_cost and duplicates dropped based on the subset of these 5 columns:

            complexity  comm_cost  optimality  simplicity  informativeness
        37131           2    0.93056    0.960774          -1          0.06944
        62              1    0.93056    0.999667           0          0.06944
        63814          16    0.92708    0.580238         -15          0.07292
        29              8    0.92708    0.757405          -7          0.07292
        61              4    0.92667    0.875317          -3          0.07333
        ...           ...        ...         ...         ...              ...
        60246          32    0.12500   -1.829808         -31          0.87500
        63821          32    0.10417   -1.829386         -31          0.89583
        60797          28    0.10417    0.926432         -27          0.89583
        61673          26    0.10417    0.998619         -25          0.89583
        44944          28    0.00000    1.000000         -27          1.00000
        [35712 rows x 5 columns]

        - A next step is to find out how I could have obtained an informativeness of 0.06944. I searched the json files, and found the two languages with these values: 

        "62": {
            "meanings": {
            "0": {
                "[[1 1 1]\n [1 1 1]]": "(1 )"
            }
            },
            "complexity": 1,
            "informativeness": 0.06944,
            "nauze": 0.0,
            "vanderklok": false,
            "iff": 1.0,
            "natural": false,
            "name": null
        },

        "50296": {
            "meanings": {
            "0": {
                "[[1 1 1]\n [1 1 1]]": "(1 )"
            },
            "1": {
                "[[1 1 1]\n [1 1 1]]": "(1 )"
            }
            },
            "complexity": 2,
            "informativeness": 0.06944,
            "nauze": 0.0,
            "vanderklok": false,
            "iff": 1.0,
            "natural": false,
            "name": null
        }, 

        - Intuitively, these languages should have informativity 0.167 for literal agents. I checked the half_credit_literal languages for this maximally ambiguous, "62" language, and got the following:

        - sampled_lang_13537:
            data:
            Language: artificial
            comm_cost: 0.5833333333333333
            complexity: 1
            dlsav: false
            iff: 1.0
            informativity: 0.4166666666666667
            name: sampled_lang_13537
            optimality: 0.9999611177526244
            sav: 0.0
            simplicity: null
            expressions:
            - form: dummy_form_62
            lot: (1 )
            meaning:
            - strong+deontic
            - strong+circumstantial
            - weak+epistemic
            - strong+epistemic
            - weak+circumstantial
            - weak+deontic

        So my math was wrong (don't feel bad, Shane also forgot that we have to be careful when thinking about indicator vs half_credit: our slack thread:
            "Perfect!! And thinking about u() in half credit: 1/6 of the time, u will be 1, 3/6 of the time it will be 0.5, and 2/6 of the time it will be 0. [So we have]

            = 1/6 + 1/4 = 5/12 = 0.4166
            
            So your implementation is right :)"
        )

        - This means conclusively that the old modals codebase is incorrect, at least with respect to computing informativity for half_credit. It's also probably incorrect wrt literal, too. Unfortunately this buggy and misleading result is in the abstract we submitted, and it's on my website as the poster: but we still have time to correct the SALT version, and I'm happy to change the poster to the talk version from salt :) 


7.3.22: print statements not showing up in evolutionary algorithm script
    This is also an old bug that I forgot about, and noticed because I want output telling me when the estimation of the frontier is starting and the sampling has stopped.

    I have the following print statements

    print("Estimating pareto frontier ...")

    print("Sampling seed generation...")

    In my estimate_pareto_frontier.py, but they're not showing up in my system.output.txt file. Come to think of it, I actually don't see the typical 

    "Saved N sampled languages" printed from my save_languages util function. 

    Weird: I actually do see these print statements, but not until after the long thing I wanted to be warned was starting, finished, which is not useful. To clarify: I get all the print statements I put in, but not at the time I expected them.