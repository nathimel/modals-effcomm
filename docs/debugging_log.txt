The purpose of this file is to track bugs that often recur, and document how they were resolved.

6.29.22: incomplete looking plot

The high informativity, low communicative cost languages don't seem to be getting explored after some updates, most recently to altk's optimization and agent modules. The space used to be fully explored. I have seen this problem before too. 
Tried:
    Looking into AddPoint:
    - uncommenting AddPoint() as a mutation to increase informativity, because at some point previously it wasn't helping -- presumably because high informativity space was getting explored fine.
    - looking into how informativity is measured in AddPoint. This doesn't matter so much as long as informativity isn't already recorded as 1.0, because it just searches the language for a point not yet covered, and then adds an expression to cover exactly that point.
    - running optimization with only AddPoint as a mutation, but this doesn't work because apparently you need all mutations in order for there to be a possible mutation when sample_mutate() is called.

    Looking into the history of some of the plots on github:
    - Looks like I had a similar issue when testing out various mutations, and found that RemovePoint was the culprit; apparently removing it as a possible mutation allowed the high informativity space to be explored. But it's possible that this wasn't the only cause, or at least something else may have motivated me to keep RemovePoint, because it still exists in the codebase.
        - Sure enough, commenting it out didn't improve on dev setting or a full run on indicator_literal. 

    - with the pragmatic settings, the effect is less dramatic, but there's still a surprising lack of exploration. I know this because I added a highly complex, but still perfectly informative language (one with many synonyms) and the points in between this and the perfect informative language with no synonyms were not found.


6.29.22: random seed not producing deterministic behavior

I have never seen deterministic behavior yet, but making the entry now. All scripts call the set_seed() function, which updates the only two libraries I know of that I'm using that use randomness:
    - random
    - numpy

7.1.22: pandas datatype confusion seems to not be a problem anymore. 

I'm not particularly worried about removing the logic which prevented floats from being read as strings, because the statistics and data look the same with and without. For reference though, pandas was once confused about there being mixed data types and I had to us pd.to_numeric() to convert all the appropriate columns back to floats.

7.1.22: strange plot with correct data but huge complexity limit at like 3e10

This happened once right after deleting altk's complexity module for refactoring, but then I haven't been able to reproduce again.


7.3.22: Highly complex DLSAV languages

    Looking into specific languages:
    - This has been an issue ever since I added dlsav functionality, and I should have noticed it before, but I am now addressing it as part of the more general debugging process of aligning / comparing the new and old codebase results.
    - There are far more vanderkok / dlsav languages with high complexity (> 35) in the new codebase than in the old one. So either I'm sampling in a very different way, or I'm measuring dlsav incorrectly. So I started inspecting the highest complexity, dlsav languages of the new codebase. Comparing half_credit_literal, I found 7 languages over 35 complexity that were dlsav, and all of them had complexity 36, except for the one highly synonymous language I added as a sanity check. Looking at the first one, `sampled_lang_1881` here were the meanings: 

    >>> for e in lang.expressions:
    ...     print(e.meaning)
    ... 
    ['strong+deontic']
    ['strong+deontic', 'strong+circumstantial'] *
    ['strong+epistemic']
    ['strong+epistemic', 'strong+circumstantial']
    ['weak+circumstantial']
    ['weak+circumstantial', 'strong+circumstantial'] *
    ['weak+deontic']
    ['weak+deontic', 'weak+circumstantial']
    ['weak+epistemic', 'strong+epistemic']
    ['weak+epistemic', 'weak+circumstantial']

    I then used their array representation to check whether the old codebase thinks this is a dlsav language. And sure enough! I found that two expressions were indeed violating the dlsav criterion, but had counted in the new codebase as passing the criterion. To see why, look at the two lists with asterisks * after them: we have two kinds of ambiguity within the root domain: one expression has fixed circumstantial flavor, but is variable-force, whereas another expression has strong force, but variable-flavor across circumstantial and deontic. 

    I ported the numpy array logic from the old codebase instead of working with the more general meaning representation as a list of strings, and 
    then the new and old codebases agreed that this language was not a dlsav language. After running the new code, the plot still looked dlsav languages were too complex -- or at least they weren't isolated to the frontier enough. This time there were only two languages with complexity > 35: `sampled_lang_1873` and `Synonymy`. Here's sampled_lang_1873:

    ['strong+circumstantial']
    ['strong+deontic']
    ['strong+circumstantial', 'strong+deontic']
    ['strong+circumstantial', 'strong+epistemic']
    ['strong+circumstantial', 'strong+deontic', 'strong+epistemic']
    ['weak+circumstantial']
    ['weak+deontic']
    ['weak+circumstantial', 'weak+deontic']
    ['weak+epistemic', 'strong+epistemic']
    ['weak+epistemic', 'weak+deontic'] 

    There are no violations of dlsav with this language. So something may have gone wrong with the logic integrating numpy arrays with the new altk meanings, or sampling is weird. I checked with the old codebase, and sure enough it concurs that this is a vanderklok_ok language. I squinted at the plots again, and realized that the scale on the y-axis was different, which is what makes the difference in complexity look so dramatic. Because this language only has complexity 36, and sure enough the old codebase plot does indeed have a dlsav language of higher than 35 complexity.

    Given this, there are actually some more serious problems, guided by the fact that the old plot has dlsav languages highly clustered along the frontier, _and_ all the very unnnatural (0 degree nauze) langs are at the high comm_cost side of the plot (around 0.8). In contrast, all of the unnatural (degree nauze) languages in the new codebase are at about 0.5 informativity. In the old codebase, 0.5 informativity has medium-naturalness, and the huge stack of dark blue languages don't start until about 0.75 and 0.8: furthermore, it looks like at least half of all the languages sampled / explored are in this high comm_cost region. This region is extremely unexplored in the new codebase. This _could_ be because of the sample size: in the new codebase, the same parameter settings result in 5659 total langs, whereas `67962` in old -- a factor of more than 10. 

        - Ran with a (random, not generation) sample size of 15k, and increased generations from 200 to 400. Resulted in 16912 total langs. The trend looks the same: dark blue stack of langs are at 0.5, rather than 0.8.

        - Okay, here's something worrying / reassuring: I printed out the dataframe of the old codebase to check out these high comm_cost languages. The simplicity measure is negative, which is a big logic error. It may still be that informativity was computed correctly, but it should be checked; after all, the we know the minimum informativity for literal speakers is exactly the prior (typically 0.1667 because I use uniform and a space of (2,3)).

        - Here's the dataframe, sorted by comm_cost and duplicates dropped based on the subset of these 5 columns:

            complexity  comm_cost  optimality  simplicity  informativeness
        37131           2    0.93056    0.960774          -1          0.06944
        62              1    0.93056    0.999667           0          0.06944
        63814          16    0.92708    0.580238         -15          0.07292
        29              8    0.92708    0.757405          -7          0.07292
        61              4    0.92667    0.875317          -3          0.07333
        ...           ...        ...         ...         ...              ...
        60246          32    0.12500   -1.829808         -31          0.87500
        63821          32    0.10417   -1.829386         -31          0.89583
        60797          28    0.10417    0.926432         -27          0.89583
        61673          26    0.10417    0.998619         -25          0.89583
        44944          28    0.00000    1.000000         -27          1.00000
        [35712 rows x 5 columns]

        - A next step is to find out how I could have obtained an informativeness of 0.06944. I searched the json files, and found the two languages with these values: 

        "62": {
            "meanings": {
            "0": {
                "[[1 1 1]\n [1 1 1]]": "(1 )"
            }
            },
            "complexity": 1,
            "informativeness": 0.06944,
            "nauze": 0.0,
            "vanderklok": false,
            "iff": 1.0,
            "natural": false,
            "name": null
        },

        "50296": {
            "meanings": {
            "0": {
                "[[1 1 1]\n [1 1 1]]": "(1 )"
            },
            "1": {
                "[[1 1 1]\n [1 1 1]]": "(1 )"
            }
            },
            "complexity": 2,
            "informativeness": 0.06944,
            "nauze": 0.0,
            "vanderklok": false,
            "iff": 1.0,
            "natural": false,
            "name": null
        }, 

        - Intuitively, these languages should have informativity 0.167 for literal agents. I checked the half_credit_literal languages for this maximally ambiguous, "62" language, and got the following:

        - sampled_lang_13537:
            data:
            Language: artificial
            comm_cost: 0.5833333333333333
            complexity: 1
            dlsav: false
            iff: 1.0
            informativity: 0.4166666666666667
            name: sampled_lang_13537
            optimality: 0.9999611177526244
            sav: 0.0
            simplicity: null
            expressions:
            - form: dummy_form_62
            lot: (1 )
            meaning:
            - strong+deontic
            - strong+circumstantial
            - weak+epistemic
            - strong+epistemic
            - weak+circumstantial
            - weak+deontic

        So my math was wrong (don't feel bad, Shane also forgot that we have to be careful when thinking about indicator vs half_credit: our slack thread:
            "Perfect!! And thinking about u() in half credit: 1/6 of the time, u will be 1, 3/6 of the time it will be 0.5, and 2/6 of the time it will be 0. [So we have]

            = 1/6 + 1/4 = 5/12 = 0.4166
            
            So your implementation is right :)"
        )

        - This means conclusively that the old modals codebase is incorrect, at least with respect to computing informativity for half_credit. It's also probably incorrect wrt literal, too. Unfortunately this buggy and misleading result is in the abstract we submitted, and it's on my website as the poster: but we still have time to correct the SALT version, and I'm happy to change the poster to the talk version from salt :) 

7.3.22 Manually check some SAV / Nauze languages for old and new codebase.

    In light of the progress I made in manually checking DLSAV / vanderklok languages, I will now check languages that seem to differ in their values and or position in the old vs new codebase, and see which implementation is right. 

    - It's a bit hard to find a good distinguishing language to do this for. The main reason we're looking into this was the shockingly low correlation of the degree-universals with simplicity: often below 0.1, and sometimes right at 0.0. At this time, simplicity correlation seems to be higher for SAV, but it's hard to know how much this is just due to chance. (And sadly, since I don't know why my random seeding logic isn't producing deterministic behavior, these results may not be exactly reproducible.)

    - One thing is that in the new codebase, there are two maximally comm_cost languages: one is a DLSAV language, and another looks like it has 0 degree SAV. It looks like they have the same comm_cost; the DLSAV one is right below it. Neither are at the frontier.
    
    In contrast, in the old codebase, the max comm_cost langs seem to be low-degree SAV languages, and they're very close to the frontier. Specifically, for the languages in this area on the frontier, the maximum comm cost language seems to be around 50% SAV, and the next most comm_cost languages are DLSAV ones. So, the strategy I think is to examine exactly these four languages, and check what their data ought to be.

    New codebase:

        - Here are the two languages (you have to drop duplicates first)


            informativity  complexity  dlsav  sav                name
        3821       0.166667           8  False  0.0  sampled_lang_13698
        1290       0.166667           4   True  1.0  sampled_lang_13618

        - As expected, informativity is minimized at 0.1667, the uniform prior probability. Note that this is DIFFERENT from the discussion above about the maximally ambiguous language's informativity value, which is 0.4166 -- maximally ambiguous does NOT mean highest communicative cost!! This is because a language could contain a single expression that is in fact _unambiguous_, but only covers a single meaning point. Then the informativity is just the probability of that meaning point, since:
            inf = sum_m p(m) * sum_e p(e | m) sum_m p(m' | e) u(m, m')
                = 1/6 + 0 + 0 + 0 + 0 + 0
                = 0.1667

        since the above values in the summation are 0 unless the m from the prior and the m' from guessing the expression are equal, which is just once. 

        Here are those two languages as saved in the new codebase:

        - sampled_lang_13698:
            data:
            Language: artificial
            comm_cost: 0.8333333333333334
            complexity: 8
            dlsav: false
            iff: 0.0
            informativity: 0.16666666666666666
            name: sampled_lang_13698
            optimality: 0.8621938064171777
            sav: 0.0
            simplicity: null
            expressions:
            - form: dummy_form_19
            lot: (+ (* (weak ) (deontic )) (* (strong ) (epistemic )))
            meaning:
            - weak+deontic
            - strong+epistemic

        - sampled_lang_13618:
            data:
            Language: artificial
            comm_cost: 0.8333333333333334
            complexity: 4
            dlsav: true
            iff: 1.0
            informativity: 0.16666666666666666
            name: sampled_lang_13618
            optimality: 0.8989421084450748
            sav: 1.0
            simplicity: null
            expressions:
            - form: dummy_form_31
            lot: (* (weak ) (epistemic ))
            meaning:
            - weak+epistemic

        The first language has the array representation:
        [[0., 1., 0.],
         [1., 0., 0.]]
        
        The second language has the array representation:
        [[1., 0., 0.],
         [0., 0., 0.]]

        So the SAV and DLSAV predictions are true for the new codebase. Furthermore, I checked the old codebase functions, and they agree. This means probably that wherever these two languages are (if they are on the the old codebase plot, they stand in the right relation to each other.

    Old codebase:

        Here are the first 3 most comm_cost languages. The first two I already discussed, but the third one is sticking out in the plot so I'm happy to look at it:

            complexity  comm_cost  informativeness  optimality       SAV  DL-SAV
    37131           2    0.93056          0.06944    0.960774  0.000000   False
    62              1    0.93056          0.06944    0.999667  0.000000   False
    63814          16    0.92708          0.07292    0.580238  0.000000   False

    Taking a look at the json files:

    "37131": {
        "meanings": {
        "0": {
            "[[0 1 1]\n [0 0 0]]": "(* (Q_1 ) (+ (f_2 ) (f_3 )))" # comp 6
        },
        "1": {
            "[[0 0 1]\n [0 0 0]]": "(* (Q_1 ) (f_3 ))" # comp 4
        },
        "2": {
            "[[1 1 0]\n [1 0 0]]": "(+ (f_1 ) (* (Q_1 ) (f_2 )))" # comp 6
        }
        },
        "complexity": 16,
        "informativeness": 0.22454,
        "nauze": 0.6666666666666666,
        "vanderklok": false,
        "iff": 0.6666666666666666,
        "natural": false,
        "name": null
    },    

    There is an error somewhere. The complexity recorded here looks good. But in the dataframe its 2! Furthermore, the informativity is not the same as in the dataframe -- 0.22454 vs 0.06944 !! 

    Also, "nauze" is supposed to be EXACTLY what SAV is. It looks like however I'm converting the names is lossy and I'm losing important data. Unfortunately, "nauze" is not in the dataframe, and the languages saved to pickle format are deleted (because github always makes me). The prediction on DL-SAV looks correct though, given the expression "2" that is both variable force and variable flavor.

    As far as the statistics goes, though, I'm confident that it's "nauze" being computed and not "SAV" because of these lines in the util_analyize code:

        predictor = "nauze"
        for prop in properties:
            r, _ = scipy.stats.pearsonr(data[prop], data[predictor])
    
    where data is a DataFrame and properties is a list of strings.

    Upshot: This data is getting messed up probably at multiple points in the code, but the complexity error seems serious, since it seems to be determining closeness to frontier in the plot at least.

    The other languages:

        "62": {
            "meanings": {
            "0": {
                "[[1 1 1]\n [1 1 1]]": "(1 )"
            }
            },
            "complexity": 1,
            "informativeness": 0.06944,
            "nauze": 0.0,
            "vanderklok": false,
            "iff": 1.0,
            "natural": false,
            "name": null
        },

    We've seen this language above before-- see the previous entry on informativity.

        "63814": {
            "meanings": {
            "0": {
                "[[0 0 0]\n [1 0 0]]": "(* (Q_2 ) (f_1 ))"
            },
            "1": {
                "[[0 1 1]\n [0 1 1]]": "(+ (f_3 ) (f_2 ))"
            },
            "2": {
                "[[0 0 0]\n [1 0 1]]": "(* (Q_2 ) (+ (f_1 ) (f_3 )))"
            }
            },
            "complexity": 14,
            "informativeness": 0.21875,
            "nauze": 0.6666666666666666,
            "vanderklok": false,
            "iff": 1.0,
            "natural": false,
            "name": null
        },
    
    Here the complexity seems correct, but clearly is getting lost by the time its put in the dataframe. The problem is, I don't know if I'm actually looking at the languages that I think I'm looking at in the plot. So I'm running the old code again. This took about 25 mins.
        - But! I think I can avoid github yelling at me for large files now because it looks like I've successfully added all .pickle files to .gitignore: with **/*.pickle.

        - Okay, in investigating some errors when trying to run this code, I'm noticing that I only seem to be using the json version of save languages for the dominant languages. I think this was probably because any pool of langs larger than this was too large for json, which is why I moved to pickle.
            - CORRECTION: no, in the analysis step, it seems that all.json does get written to! In particular, it is written to early on, right after unpickling and turning into a dataframe: 

                all_langs = random_langs + nauze_langs + dom_langs + evolutionary_path_langs # + natural_langs
                all_langs = unique_languages_by_vocab(all_langs, loading_bar=True)
                data = get_dataframe(all_langs)
                save_languages(all_langs, 'all.json')

            So all.json is actually probably adequate for some inspection.
            There's nothing spooky about the `get_dataframe` function; it seems to be simply measuring languages using pooling, and then mapping them into a dataframe.
            Unfortunately, due to randomness I can't inspect the exact languages I looked at above, '37131' shows a different language now in all.json.

            what could have been bad was a section in analyze.py where I measure the pareto front again. I suspect I did this because it smooths the curve; but I've commented it out (as I remember doing back and forth many times earlier this year). 
                - update: this doesn't seem to have done anything except cause interpolate to throw an error complaining about div by zero.

            The other thing could be the `rename_author_columns` function, which is super ugly and I'm willing to give it up since I'm not going to use this code to produce official figures anymore.

        - actually, the error I got was this:

            Saving languages ...
            0%|                                                                                                        | 0/72 [00:00<?, ?it/s]
            Traceback (most recent call last):
            File "/Users/nathanielimel/clms/thesis/code/modals/src/analysis/analyze.py", line 90, in <module>
                main()
            File "/Users/nathanielimel/clms/thesis/code/modals/src/analysis/analyze.py", line 57, in main
                save_languages(dominating_languages, 'dominant.json') # idk why I was indexing all_langs
            File "/Users/nathanielimel/clms/thesis/code/modals/src/file_util.py", line 92, in save_languages
                attributes["meanings"] = {i:{np.array2string(np.array(item_dict["arr"])): item_dict["string"]} for i, item_dict in enumerate(L.lexicon_)}
            AttributeError

        I recently changed the following line

            save_languages([all_langs[i] for i in dominating_indices], 'dominant.json') # overwrite dominant
        to 

            save_languages([all_langs[i] for i in dominating_indices], 'dominant.json') # overwrite dominant

        because I immediately above had

            dominating_languages = [languages[i] for i in dominating_indices]

        But it looks like I shouldn't have lol. Because here's where `languages` gets initialized:

            languages = pareto_raw_df.values.tolist()

        e.g, it's just a list of languages _as their pareto data_, instead of as language objects.

    When this code finally finished running, the results -- report and plot-- look exactly as before. This surprised me, because I uncommented the scaling complexity line, so I would have thought simplicity would have been changed. But complexity and simplicity are perfect -1 relationship, and the correlations are the same. Looking at the dataframe for all languages, I see that simplicity is no longer negative, but we have basically the same languages:

            complexity  comm_cost  simplicity  informativeness
        62       0.011364    0.93056    0.988636          0.06944
        36150    0.022727    0.93056    0.977273          0.06944
        36101    0.227273    0.92708    0.772727          0.07292
        28       0.113636    0.92708    0.886364          0.07292
        39670    0.181818    0.92667    0.818182          0.07333

    Without scaling the data, and commenting out the plot and df lines that rename the author columns, I get:

            complexity  informativeness  nauze
        62              1          0.06944    0.0
        55              2          0.11111    1.0
        8               2          0.12500    1.0
        36150           2          0.06944    0.0
        1994            3          0.10764    0.5
        ...           ...              ...    ...
        31916          84          0.12896    0.0
        16700          86          0.16835    0.2
        32748          86          0.13764    0.1
        32495          86          0.16305    0.1
        17601          88          0.14736    0.1
    
    Which all seems fine. At this point I may focus on more explicitly engineered debugging investigation.


7.3.22: print statements not showing up in evolutionary algorithm script
    This is also an old bug that I forgot about, and noticed because I want output telling me when the estimation of the frontier is starting and the sampling has stopped.

    I have the following print statements

    print("Estimating pareto frontier ...")

    print("Sampling seed generation...")

    In my estimate_pareto_frontier.py, but they're not showing up in my system.output.txt file. Come to think of it, I actually don't see the typical 

    "Saved N sampled languages" printed from my save_languages util function. 

    Weird: I actually do see these print statements, but not until after the long thing I wanted to be warned was starting, finished, which is not useful. To clarify: I get all the print statements I put in, but not at the time I expected them.

7.3.22: Negative simplicity values in the old codebase:

    This is probably because I had the following line commented out:

        data = scale_complexity(data, max(list(data["complexity"])))

    And without this, in the function `add_columns`, simplicity is defined as 1 - complexity, but if complexity isn't in [0,1], we'll get negative values.

7.3.22: At this point, I will try to find the truth about the complexity correlations with naturalness by building something that can take the languages of the old codebase and turn them into equivalent altk / new codebase languages.

    The motivation is the following: I've checked the measures of naturalness (SAV / Nauze), and the complexity values of languages look good upon inspection of the old codebase. So we have (implicitly) two different datasets: languages with their complexity and naturalness data. These define 2 correlation coefficients. The task is to make sure these datasets are truly comparable, and then find the right coefficient.

    1. Implement tools to convert between old languages and the new ones.
    2. Run the complexity measures again on all the languages.
    3. Get the pearson correlations.

7.4.22: Writing the debug_correlations.py script

    So far this is a promising strategy. I load up the old languages as all.json, and then construct the analogous ModalLanguages. Along the way I keep track of any discrepancies between complexity in the old and new language, and between the old and new measure of naturalness. So far, no discrepancies for naturalness. I did get, however:

    With negation off in old but on in new:

        total discrepancies:  58251
        total old languages:  66321
        Saved 66321 old languages

    Upon closer inspection, though, I realized this is probably because I'm comparing two differnt complexity measures: one with negation, and one without. So next I will measure will negation turned off in the new codebase, and then run the old code with negation on and measure that too.

        - Trying this, I noticed that the new langs still had negation. So i'm looking into the modal_language_of_thought module for errors not following the configs and such.
            - Changed the line
                self.contains_negation = "negation" in lot_configs
            to 
            self.contains_negation = lot_configs["negation"]

    With negation off in both old and new:

        total discrepancies:  0
        total old languages:  66321
        Saved 66321 old languages

    Furthermore, here was the correlation data of naturalness with complexity, which is indeed quite high:

            sample  pearson
            new     -0.4685788796530204
            old     -0.4685788796530204
    
    Given this, I want to measure the tradeoff and do analysis in the new codebase with these converted old languages. My best guess is that something has happened with sampling so that the old languages represent a better sampling of the space, or at least one that results in more positive results.
    That is: 
    
        - I run the pipeline of the full experiment, but instead of sampling and using the evolutionary algorithm, I simply let the pool of languages consist of the old langauges converted to ModalLanguages. After all, those languages have been found by exploration with the evolutionary algorithm and sampling from the old codebase.

    One thing I am noticing is that all the langauges are being kept even after running set() on them to unique, but I doubt they really are all unique. I'm not sure yet why, because the expression names should just be the same as all expressions used in the new codebase. Looking at the hash function for ModalLanguage, I see as expected:

        # requiring diff name is a strong requirement
        return hash(tuple(sorted(self.expressions)))

    which ought to be uniquing things. But, in analysis, I still have the option of dropping duplicates, so I can probably be confident about not counting extras there.

    Here were the results, which concur with the old codebase!

        Degree sav pearson correlations:
        simplicity: 0.46870328674046224
        complexity: -0.4687032867404632
        informativity: 0.492507973867846
        optimality: 0.5571371626617981

        MEANS
                        simplicity  complexity  informativity  optimality
        name                                                               
        natural_means       0.813636   16.400000       0.746296    0.957112
        dlsav_means         0.813636   16.400000       0.746296    0.957112
        population_means    0.634684   32.147834       0.532526    0.783940

        TTEST STATS
        natural languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic          9.842396   -9.842396       3.022135    7.417872
        Two-sided p-value    0.000598    0.000598       0.039081    0.001763

        dlsav languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic          9.842396   -9.842396       3.022135    7.417872
        Two-sided p-value    0.000598    0.000598       0.039081    0.001763

    The plot does look like more of the space is explored (in particular, the high informativity langs look more explored). But it's not dramatically different from the new plots i'm used to looking at at this point. I note that the informativity looks right, unlike in the old plot -- which had values past the prior. In fact, the similarity is so striking that I've 
    saved the two plots in a folder called `docs/first_correlation_debug_run`.

    This is without dropping duplicates, and the length of all_data.csv is 66325 without 5 the natural languages.
    These results can be found on the commit message "first correlation run".

    When I drop duplicates. The results don't change much. The all_data.csv has about half as many entries, 35170 + 5.

        Degree sav pearson correlations:
        simplicity: 0.4401090069250126
        complexity: -0.44010900692501265
        informativity: 0.6399118603515791
        optimality: 0.5432094201556613

        MEANS
                        simplicity  complexity  informativity  optimality
        name                                                               
        natural_means       0.813636   16.400000       0.746296    0.957112
        dlsav_means         0.813636   16.400000       0.746296    0.957112
        population_means    0.516987   42.505159       0.530802    0.708255

        TTEST STATS
        natural languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         16.315724  -16.315724       3.046501   10.659846
        Two-sided p-value    0.000083    0.000083       0.038159    0.000439

        dlsav languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         16.315724  -16.315724       3.046501   10.659846
        Two-sided p-value    0.000083    0.000083       0.038159    0.000439    

    - Next, I plan to run the same steps, but instead of keeping the new codebase sampled languages out of the analysis, I will let the pool of languages consist of all the old languages plus the new codebase sampled and explored ones.

    Here were the results:

        Degree sav pearson correlations:
        simplicity: 0.4151781617085524
        complexity: -0.4151781617085524
        informativity: 0.6770489229218878
        optimality: 0.5256428747463577

        MEANS
                        simplicity  complexity  informativity  optimality
        name                                                               
        natural_means       0.804545   17.200000       0.746296    0.955460
        dlsav_means         0.804545   17.200000       0.746296    0.955460
        population_means    0.517221   42.484588       0.533966    0.706636

        TTEST STATS
        natural languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         13.632536  -13.632536       3.001778   10.882309
        Two-sided p-value    0.000168    0.000168       0.039872    0.000405

        dlsav languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         13.632536  -13.632536       3.001778   10.882309
        Two-sided p-value    0.000168    0.000168       0.039872    0.000405

    You can view the results on the commit message "pooled old and new langs together in one experiment".

    For sanity, I'm also running the new code with negation turned off only, and no langs added from the old codebase. I've done this many times already, but I just want to check that the correlations are still quite low.
        - Interestingly, doing this resulted in an error in analysis.py in the call:
           data = get_dataframe(langs, **kwargs)
           .
           .
           .
            ValueError: Length of values (2528) does not match length of index (2529)

        Looking at the altk analysis submodule, I see

            # drop but count duplicates
            elif duplicates == "count":
                vcs = data.value_counts(subset=subset)
                data = data.drop_duplicates(subset=subset)
                data = data.sort_values(by=subset)
                data["counts"] = vcs.values

        I checked this logic on small toy data in a python environment, but it seemed correct.

        - RESOLVED: I looked at the data and there were many Nones and NaNs. I realized this was because I was loading languages that weren't measured: this was because I forgot to uncomment out the save_languages lines at the end of measure_tradeoff (which I had replaced with different saving lines for the above experiment combining old and new)!

    And sure enough, here are the very low correlations.

        Degree sav pearson correlations:
        simplicity: 0.0805049152987793
        complexity: -0.08050491529877928
        informativity: 0.7342289479023422
        optimality: 0.3294735688942782

        MEANS
                        simplicity  complexity  informativity  optimality
        name                                                               
        natural_means       0.800000   17.200000       0.746296    0.958545
        dlsav_means         0.800000   17.200000       0.746296    0.958545
        population_means    0.638135   31.120363       0.580944    0.807325

        TTEST STATS
        natural languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic          7.505357   -7.505357       2.337639    6.176596
        Two-sided p-value    0.001686    0.001686       0.079584    0.003490

        dlsav languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic          7.505357   -7.505357       2.337639    6.176596
        Two-sided p-value    0.001686    0.001686       0.079584    0.003490    


    Though this is _much_ higher correlation with informativity than I usually see. Don't know what could be going on here. Could be the lack of negation, but that doesn't make much sense because that should only impact complexity.


    Now I will run BOTH full experiments with negation turned on. This involves the following steps:
        - running the old code (less than 25 mins)
        - obtaining the json files lexicon.json, all.json
        - running the debug_correlations.py script and checking all the measurements are the same
        - deleting any languages saved in output folders
        - running the measure_tradeoff and analysis scripts as one check (just old langs)
        - running the entire pipeline, as a separate check (pooling the old and new langs)

    No discrepancies were found between the old and new language measurements:

        total discrepancies:  0
        total old languages:  66456
        Saved 66456 old languages

        sample  pearson
        new     -0.4091872227754934
        old     -0.4091872227754934

    Check 1: The old languages alone

    In the final analysis, I see that the correlation is high, but not as high as the debug script predicted (0.38 vs 0.40), which is strange, but could be due to dropping duplicates in the analysis script (indeed, looking at all_data, its only about 30k long instead of 66k).

        Degree sav pearson correlations:
        simplicity: 0.38204573364941424
        complexity: -0.38204573364941447
        informativity: 0.6553656982496618
        optimality: 0.5072446445166654

        MEANS
                        simplicity  complexity  informativity  optimality
        name                                                               
        natural_means       0.789744   16.400000       0.746296    0.955218
        dlsav_means         0.789744   16.400000       0.746296    0.955218
        population_means    0.556643   34.581826       0.531503    0.737487

        TTEST STATS
        natural languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         11.363641  -11.363641       3.036599    7.871290
        Two-sided p-value    0.000342    0.000342       0.038530    0.001408

        dlsav languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         11.363641  -11.363641       3.036599    7.871290
        Two-sided p-value    0.000342    0.000342       0.038530    0.001408 

    Check 2: the old langs combined with the new codebases pool of langs

    Here were the results: slightly lower complexity as expected,

        Degree sav pearson correlations:
        simplicity: 0.3568978222203001
        complexity: -0.3568978222203005
        informativity: 0.6823818567868
        optimality: 0.494225008839281

        MEANS
                        simplicity  complexity  informativity  optimality
        name                                                               
        natural_means       0.789744   16.400000       0.746296    0.953079
        dlsav_means         0.789744   16.400000       0.746296    0.953079
        population_means    0.556636   34.582355       0.533804    0.737567

        TTEST STATS
        natural languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         11.363972  -11.363972       3.004070    7.912685
        Two-sided p-value    0.000342    0.000342       0.039782    0.001380

        dlsav languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         11.363972  -11.363972       3.004070    7.912685
        Two-sided p-value    0.000342    0.000342       0.039782    0.001380

    Nothing too dramatic in the plot, other than the high informativity space looks better.

7.4.22: Taking samples of JUST 100% natural languages.

    I notice that in the old codebase this is something I did. I think this is a nice strategy. It raises the question of whether the sample is biased, and thus affecting correlations. In general, we really don't know how to take a good sample of the mathematically possible languages. 
        - Uniform sampling is really bad at finding optimal languages. 
        - And the evolutionary algorithm should be good at finding those optimal ones, but there may be some lock-in involved -- though we do randomly mutate the parents and save them to the explored langs, regardless of their local fitness, so I'm not sure why this is such a problem.

    Doing this in the new codebase:

        - Unfortunately, while passing in only the natural expressions lets the generate_languages() function run just fine, it doesn't seem to be finding any langauges that the normal random sampler can't find. I know this because it's saving the same number before and after.
        In particular, we're back to almost 0 correlation again:
                    
            Degree sav pearson correlations:
            simplicity: -0.030188734478632562
            complexity: 0.030188734478632653
            informativity: 0.7449740074285384
            optimality: 0.26938205710854785        

        - HOWEVER! I turned up the knob on sample size from 2k to 10k, and got

            Degree sav pearson correlations:
            simplicity: 0.23183545729787725
            complexity: -0.23183545729787725
            informativity: 0.7811426304586019
            optimality: 0.46861543248553206        

        Which is far higher. This motivates turning up the sample size even higher, and also thinking about seeding the evolutionary algorithm with more degree 1.0 universal_property languages.
        
        The commit name is "Correlation with simplicity went way up as a result of sampling fully SAV languages and increasing sample size", which, lmao, sounds like p-hacking if I ever heard of it xD. But seriously, I'm not convinced that it is. P-hacking I think describes when you get a really unrepresentative sample, and the whole point is that we're exploring as best we can, possibly even better now than before. Whether there are more universal_property languages elsewhere in the space that hasn't been explored is still an open question. The _trend_ looks like the answer would be no, but at this point I'm cautious to make strong claims.

        Turning up to 20k sampling size, we get 15764 sampled languages and 20494 total langs. And the correlation with both simplicity and informativity goes up a bit:

            Degree sav pearson correlations:
            simplicity: 0.27448407524007945
            complexity: -0.2744840752400772
            informativity: 0.8034243588109712
            optimality: 0.5171836442749919

        This is for half_credit_literal. 
        Trying for indicator_literal with a sample size of 20k, we get 15764 sampled languages (which should be the same) and 20494 total langs (which could in principle be different, but not super worrying):

            Degree sav pearson correlations:
            simplicity: 0.27448407524007945
            complexity: -0.2744840752400772
            informativity: 0.8034243588109712
            optimality: 0.5171836442749919


    Adding degree 1.0 naturalness languages to the seed population of the evolutionary algorithm:

        - I actually noticed that the generate_languages function call for the seed actually starts off with _no_ criterion applied. I might change this to include the criterion to get even amounts of degrees, but since I'm going to inject with high naturalness languages anyway, I'll do the latter first.

        - the results were about the same, with slightly less languages explored during the evolutionary algorithm. That makes some sense: I gave it a head start to the extent highly natural languages are closer to the frontier.

            19634 total langs.
            .
            .
            .
            Degree sav pearson correlations:
            simplicity: 0.27942214075127864
            complexity: -0.27942214075127825
            informativity: 0.8005810203039387
            optimality: 0.5191003974244972

7.4.22: Testing with very large sampling size

    Given that I can increase the optimality correlation with naturalness by sampling more degree 1.0 languages, a natural next step which addresses both sampling more optimal languages and sampling more of the space in general, is simply to ask for a very large sample size.

    In particular, I won't inject degree 1.0 langs in sampling. There will be just one call to `generate_languages`. 

        - First, I will try 20k.

        Results for half_credit_literal look like they confirm the hypothesis: 

            18529 total langs.

            Degree sav pearson correlations:
            simplicity: 0.27478177250674074
            complexity: -0.2747817725067404
            informativity: 0.8070911122098514
            optimality: 0.5358468342428402

        Results for indicator_literal similarly (and are more positive):

            18640 total langs.

            Degree sav pearson correlations:
            simplicity: 0.29572958275332895
            complexity: -0.29572958275332895
            informativity: 0.7995081848160996
            optimality: 0.460000315343279

        And this was 15764 sampled languages, as before with injecting the degree1 langs.

        - Increasing the sample size to 40k:

            Notably, this results in 30710 sampled languages. Interestingly, the evolutionary algorithm doesn't do much to explore: it finds 33363 explored languages, and the final analysis has just 33368 total langs (add 5 for the naturals). 
                - This means that much of the work can be done by the sampling script, and not the evolutionary algorithm -- which is opposite of what I had been believing. Also, the sampling script is faster at finding languages (though this could partly be due to the fact the evolutionary algorithm measures each generation, and there's no measurement in the sampler.)
                - I also note that (after dropping duplicates) the all_data.csv has about 23k entries.

            I also started timing the full experiment in run.sh. The 40k run took 8m17.086s total. (half_credit_literal)

            Here were the results:

                Degree sav pearson correlations:
                simplicity: 0.30819300359460555
                complexity: -0.30819300359460544
                informativity: 0.8317336287804182
                optimality: 0.5668854005095101

            Again, the plot shows that the low comm_cost space is still very unexplored. I'm also going to stop printing complexity correlations until I need them again.



        One thing I do note is that I have also removed the sanity check and synonymy languages in the seeding of the evolutionary algorithm, and this still results in the low comm_cost space not being explored. Given that I can get the same results with this higher sampling number, my hypothesis is that the low comm_cost is not well explored by uniform sampling. It may have been well explored by the evolutionary algorithm at some point, but I don't know what has gone wrong. This inspires a very important conjecture:
            - The reason why I'm getting like 0.3 correlation at the highest right now, instead of the old codebase's 0.4Xs, could be because the low comm_cost space is not being explored enough. So my next hypothesis to test is:
                - Find a way to increase the high informativity space sample, because this may result in higher correlation of naturalness with optimality.