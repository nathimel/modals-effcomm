The purpose of this file is to track bugs that often recur, and document how they were resolved.

6.29.22: incomplete looking plot
status: resolved

    - NB: solved partially on 7.5.22 by exploring all corners of the space with optimizer.

The high informativity, low communicative cost languages don't seem to be getting explored after some updates, most recently to altk's optimization and agent modules. The space used to be fully explored. I have seen this problem before too. 
Tried:
    Looking into AddPoint:
    - uncommenting AddPoint() as a mutation to increase informativity, because at some point previously it wasn't helping -- presumably because high informativity space was getting explored fine.
    - looking into how informativity is measured in AddPoint. This doesn't matter so much as long as informativity isn't already recorded as 1.0, because it just searches the language for a point not yet covered, and then adds an expression to cover exactly that point.
    - running optimization with only AddPoint as a mutation, but this doesn't work because apparently you need all mutations in order for there to be a possible mutation when sample_mutate() is called.

    Looking into the history of some of the plots on github:
    - Looks like I had a similar issue when testing out various mutations, and found that RemovePoint was the culprit; apparently removing it as a possible mutation allowed the high informativity space to be explored. But it's possible that this wasn't the only cause, or at least something else may have motivated me to keep RemovePoint, because it still exists in the codebase.
        - Sure enough, commenting it out didn't improve on dev setting or a full run on indicator_literal. 

    - with the pragmatic settings, the effect is less dramatic, but there's still a surprising lack of exploration. I know this because I added a highly complex, but still perfectly informative language (one with many synonyms) and the points in between this and the perfect informative language with no synonyms were not found.


6.29.22: random seed not producing deterministic behavior
status: unresolved

I have never seen deterministic behavior yet, but making the entry now. All scripts call the set_seed() function, which updates the only two libraries I know of that I'm using that use randomness:
    - random
    - numpy

7.1.22: pandas datatype confusion seems to not be a problem anymore. 
status: resolved*

I'm not particularly worried about removing the logic which prevented floats from being read as strings, because the statistics and data look the same with and without. For reference though, pandas was once confused about there being mixed data types and I had to us pd.to_numeric() to convert all the appropriate columns back to floats.

7.1.22: strange plot with correct data but huge complexity limit at like 3e10
status: resolved*

This happened once right after deleting altk's complexity module for refactoring, but then I haven't been able to reproduce again.


7.3.22: Highly complex DLSAV languages
status: resolved

    Looking into specific languages:
    - This has been an issue ever since I added dlsav functionality, and I should have noticed it before, but I am now addressing it as part of the more general debugging process of aligning / comparing the new and old codebase results.
    - There are far more vanderkok / dlsav languages with high complexity (> 35) in the new codebase than in the old one. So either I'm sampling in a very different way, or I'm measuring dlsav incorrectly. So I started inspecting the highest complexity, dlsav languages of the new codebase. Comparing half_credit_literal, I found 7 languages over 35 complexity that were dlsav, and all of them had complexity 36, except for the one highly synonymous language I added as a sanity check. Looking at the first one, `sampled_lang_1881` here were the meanings: 

    >>> for e in lang.expressions:
    ...     print(e.meaning)
    ... 
    ['strong+deontic']
    ['strong+deontic', 'strong+circumstantial'] *
    ['strong+epistemic']
    ['strong+epistemic', 'strong+circumstantial']
    ['weak+circumstantial']
    ['weak+circumstantial', 'strong+circumstantial'] *
    ['weak+deontic']
    ['weak+deontic', 'weak+circumstantial']
    ['weak+epistemic', 'strong+epistemic']
    ['weak+epistemic', 'weak+circumstantial']

    I then used their array representation to check whether the old codebase thinks this is a dlsav language. And sure enough! I found that two expressions were indeed violating the dlsav criterion, but had counted in the new codebase as passing the criterion. To see why, look at the two lists with asterisks * after them: we have two kinds of ambiguity within the root domain: one expression has fixed circumstantial flavor, but is variable-force, whereas another expression has strong force, but variable-flavor across circumstantial and deontic. 

    I ported the numpy array logic from the old codebase instead of working with the more general meaning representation as a list of strings, and 
    then the new and old codebases agreed that this language was not a dlsav language. After running the new code, the plot still looked dlsav languages were too complex -- or at least they weren't isolated to the frontier enough. This time there were only two languages with complexity > 35: `sampled_lang_1873` and `Synonymy`. Here's sampled_lang_1873:

    ['strong+circumstantial']
    ['strong+deontic']
    ['strong+circumstantial', 'strong+deontic']
    ['strong+circumstantial', 'strong+epistemic']
    ['strong+circumstantial', 'strong+deontic', 'strong+epistemic']
    ['weak+circumstantial']
    ['weak+deontic']
    ['weak+circumstantial', 'weak+deontic']
    ['weak+epistemic', 'strong+epistemic']
    ['weak+epistemic', 'weak+deontic'] 

    There are no violations of dlsav with this language. So something may have gone wrong with the logic integrating numpy arrays with the new altk meanings, or sampling is weird. I checked with the old codebase, and sure enough it concurs that this is a vanderklok_ok language. I squinted at the plots again, and realized that the scale on the y-axis was different, which is what makes the difference in complexity look so dramatic. Because this language only has complexity 36, and sure enough the old codebase plot does indeed have a dlsav language of higher than 35 complexity.

    Given this, there are actually some more serious problems, guided by the fact that the old plot has dlsav languages highly clustered along the frontier, _and_ all the very unnnatural (0 degree nauze) langs are at the high comm_cost side of the plot (around 0.8). In contrast, all of the unnatural (degree nauze) languages in the new codebase are at about 0.5 informativity. In the old codebase, 0.5 informativity has medium-naturalness, and the huge stack of dark blue languages don't start until about 0.75 and 0.8: furthermore, it looks like at least half of all the languages sampled / explored are in this high comm_cost region. This region is extremely unexplored in the new codebase. This _could_ be because of the sample size: in the new codebase, the same parameter settings result in 5659 total langs, whereas `67962` in old -- a factor of more than 10. 

        - Ran with a (random, not generation) sample size of 15k, and increased generations from 200 to 400. Resulted in 16912 total langs. The trend looks the same: dark blue stack of langs are at 0.5, rather than 0.8.

        - Okay, here's something worrying / reassuring: I printed out the dataframe of the old codebase to check out these high comm_cost languages. The simplicity measure is negative, which is a big logic error. It may still be that informativity was computed correctly, but it should be checked; after all, the we know the minimum informativity for literal speakers is exactly the prior (typically 0.1667 because I use uniform and a space of (2,3)).

        - Here's the dataframe, sorted by comm_cost and duplicates dropped based on the subset of these 5 columns:

            complexity  comm_cost  optimality  simplicity  informativeness
        37131           2    0.93056    0.960774          -1          0.06944
        62              1    0.93056    0.999667           0          0.06944
        63814          16    0.92708    0.580238         -15          0.07292
        29              8    0.92708    0.757405          -7          0.07292
        61              4    0.92667    0.875317          -3          0.07333
        ...           ...        ...         ...         ...              ...
        60246          32    0.12500   -1.829808         -31          0.87500
        63821          32    0.10417   -1.829386         -31          0.89583
        60797          28    0.10417    0.926432         -27          0.89583
        61673          26    0.10417    0.998619         -25          0.89583
        44944          28    0.00000    1.000000         -27          1.00000
        [35712 rows x 5 columns]

        - A next step is to find out how I could have obtained an informativeness of 0.06944. I searched the json files, and found the two languages with these values: 

        "62": {
            "meanings": {
            "0": {
                "[[1 1 1]\n [1 1 1]]": "(1 )"
            }
            },
            "complexity": 1,
            "informativeness": 0.06944,
            "nauze": 0.0,
            "vanderklok": false,
            "iff": 1.0,
            "natural": false,
            "name": null
        },

        "50296": {
            "meanings": {
            "0": {
                "[[1 1 1]\n [1 1 1]]": "(1 )"
            },
            "1": {
                "[[1 1 1]\n [1 1 1]]": "(1 )"
            }
            },
            "complexity": 2,
            "informativeness": 0.06944,
            "nauze": 0.0,
            "vanderklok": false,
            "iff": 1.0,
            "natural": false,
            "name": null
        }, 

        - Intuitively, these languages should have informativity 0.167 for literal agents. I checked the half_credit_literal languages for this maximally ambiguous, "62" language, and got the following:

        - sampled_lang_13537:
            data:
            Language: artificial
            comm_cost: 0.5833333333333333
            complexity: 1
            dlsav: false
            iff: 1.0
            informativity: 0.4166666666666667
            name: sampled_lang_13537
            optimality: 0.9999611177526244
            sav: 0.0
            simplicity: null
            expressions:
            - form: dummy_form_62
            lot: (1 )
            meaning:
            - strong+deontic
            - strong+circumstantial
            - weak+epistemic
            - strong+epistemic
            - weak+circumstantial
            - weak+deontic

        So my math was wrong (don't feel bad, Shane also forgot that we have to be careful when thinking about indicator vs half_credit: our slack thread:
            "Perfect!! And thinking about u() in half credit: 1/6 of the time, u will be 1, 3/6 of the time it will be 0.5, and 2/6 of the time it will be 0. [So we have]

            = 1/6 + 1/4 = 5/12 = 0.4166
            
            So your implementation is right :)"
        )

        - This means conclusively that the old modals codebase is incorrect, at least with respect to computing informativity for half_credit. It's also probably incorrect wrt literal, too. Unfortunately this buggy and misleading result is in the abstract we submitted, and it's on my website as the poster: but we still have time to correct the SALT version, and I'm happy to change the poster to the talk version from salt :) 

7.3.22 Manually check some SAV / Nauze languages for old and new codebase.
status: resolved

    In light of the progress I made in manually checking DLSAV / vanderklok languages, I will now check languages that seem to differ in their values and or position in the old vs new codebase, and see which implementation is right. 

    - It's a bit hard to find a good distinguishing language to do this for. The main reason we're looking into this was the shockingly low correlation of the degree-universals with simplicity: often below 0.1, and sometimes right at 0.0. At this time, simplicity correlation seems to be higher for SAV, but it's hard to know how much this is just due to chance. (And sadly, since I don't know why my random seeding logic isn't producing deterministic behavior, these results may not be exactly reproducible.)

    - One thing is that in the new codebase, there are two maximally comm_cost languages: one is a DLSAV language, and another looks like it has 0 degree SAV. It looks like they have the same comm_cost; the DLSAV one is right below it. Neither are at the frontier.
    
    In contrast, in the old codebase, the max comm_cost langs seem to be low-degree SAV languages, and they're very close to the frontier. Specifically, for the languages in this area on the frontier, the maximum comm cost language seems to be around 50% SAV, and the next most comm_cost languages are DLSAV ones. So, the strategy I think is to examine exactly these four languages, and check what their data ought to be.

    New codebase:

        - Here are the two languages (you have to drop duplicates first)


            informativity  complexity  dlsav  sav                name
        3821       0.166667           8  False  0.0  sampled_lang_13698
        1290       0.166667           4   True  1.0  sampled_lang_13618

        - As expected, informativity is minimized at 0.1667, the uniform prior probability. Note that this is DIFFERENT from the discussion above about the maximally ambiguous language's informativity value, which is 0.4166 -- maximally ambiguous does NOT mean highest communicative cost!! This is because a language could contain a single expression that is in fact _unambiguous_, but only covers a single meaning point. Then the informativity is just the probability of that meaning point, since:
            inf = sum_m p(m) * sum_e p(e | m) sum_m p(m' | e) u(m, m')
                = 1/6 + 0 + 0 + 0 + 0 + 0
                = 0.1667

        since the above values in the summation are 0 unless the m from the prior and the m' from guessing the expression are equal, which is just once. 

        Here are those two languages as saved in the new codebase:

        - sampled_lang_13698:
            data:
            Language: artificial
            comm_cost: 0.8333333333333334
            complexity: 8
            dlsav: false
            iff: 0.0
            informativity: 0.16666666666666666
            name: sampled_lang_13698
            optimality: 0.8621938064171777
            sav: 0.0
            simplicity: null
            expressions:
            - form: dummy_form_19
            lot: (+ (* (weak ) (deontic )) (* (strong ) (epistemic )))
            meaning:
            - weak+deontic
            - strong+epistemic

        - sampled_lang_13618:
            data:
            Language: artificial
            comm_cost: 0.8333333333333334
            complexity: 4
            dlsav: true
            iff: 1.0
            informativity: 0.16666666666666666
            name: sampled_lang_13618
            optimality: 0.8989421084450748
            sav: 1.0
            simplicity: null
            expressions:
            - form: dummy_form_31
            lot: (* (weak ) (epistemic ))
            meaning:
            - weak+epistemic

        The first language has the array representation:
        [[0., 1., 0.],
         [1., 0., 0.]]
        
        The second language has the array representation:
        [[1., 0., 0.],
         [0., 0., 0.]]

        So the SAV and DLSAV predictions are true for the new codebase. Furthermore, I checked the old codebase functions, and they agree. This means probably that wherever these two languages are (if they are on the the old codebase plot, they stand in the right relation to each other.

    Old codebase:

        Here are the first 3 most comm_cost languages. The first two I already discussed, but the third one is sticking out in the plot so I'm happy to look at it:

            complexity  comm_cost  informativeness  optimality       SAV  DL-SAV
    37131           2    0.93056          0.06944    0.960774  0.000000   False
    62              1    0.93056          0.06944    0.999667  0.000000   False
    63814          16    0.92708          0.07292    0.580238  0.000000   False

    Taking a look at the json files:

    "37131": {
        "meanings": {
        "0": {
            "[[0 1 1]\n [0 0 0]]": "(* (Q_1 ) (+ (f_2 ) (f_3 )))" # comp 6
        },
        "1": {
            "[[0 0 1]\n [0 0 0]]": "(* (Q_1 ) (f_3 ))" # comp 4
        },
        "2": {
            "[[1 1 0]\n [1 0 0]]": "(+ (f_1 ) (* (Q_1 ) (f_2 )))" # comp 6
        }
        },
        "complexity": 16,
        "informativeness": 0.22454,
        "nauze": 0.6666666666666666,
        "vanderklok": false,
        "iff": 0.6666666666666666,
        "natural": false,
        "name": null
    },    

    There is an error somewhere. The complexity recorded here looks good. But in the dataframe its 2! Furthermore, the informativity is not the same as in the dataframe -- 0.22454 vs 0.06944 !! 

    Also, "nauze" is supposed to be EXACTLY what SAV is. It looks like however I'm converting the names is lossy and I'm losing important data. Unfortunately, "nauze" is not in the dataframe, and the languages saved to pickle format are deleted (because github always makes me). The prediction on DL-SAV looks correct though, given the expression "2" that is both variable force and variable flavor.

    As far as the statistics goes, though, I'm confident that it's "nauze" being computed and not "SAV" because of these lines in the util_analyize code:

        predictor = "nauze"
        for prop in properties:
            r, _ = scipy.stats.pearsonr(data[prop], data[predictor])
    
    where data is a DataFrame and properties is a list of strings.

    Upshot: This data is getting messed up probably at multiple points in the code, but the complexity error seems serious, since it seems to be determining closeness to frontier in the plot at least.

    The other languages:

        "62": {
            "meanings": {
            "0": {
                "[[1 1 1]\n [1 1 1]]": "(1 )"
            }
            },
            "complexity": 1,
            "informativeness": 0.06944,
            "nauze": 0.0,
            "vanderklok": false,
            "iff": 1.0,
            "natural": false,
            "name": null
        },

    We've seen this language above before-- see the previous entry on informativity.

        "63814": {
            "meanings": {
            "0": {
                "[[0 0 0]\n [1 0 0]]": "(* (Q_2 ) (f_1 ))"
            },
            "1": {
                "[[0 1 1]\n [0 1 1]]": "(+ (f_3 ) (f_2 ))"
            },
            "2": {
                "[[0 0 0]\n [1 0 1]]": "(* (Q_2 ) (+ (f_1 ) (f_3 )))"
            }
            },
            "complexity": 14,
            "informativeness": 0.21875,
            "nauze": 0.6666666666666666,
            "vanderklok": false,
            "iff": 1.0,
            "natural": false,
            "name": null
        },
    
    Here the complexity seems correct, but clearly is getting lost by the time its put in the dataframe. The problem is, I don't know if I'm actually looking at the languages that I think I'm looking at in the plot. So I'm running the old code again. This took about 25 mins.
        - But! I think I can avoid github yelling at me for large files now because it looks like I've successfully added all .pickle files to .gitignore: with **/*.pickle.

        - Okay, in investigating some errors when trying to run this code, I'm noticing that I only seem to be using the json version of save languages for the dominant languages. I think this was probably because any pool of langs larger than this was too large for json, which is why I moved to pickle.
            - CORRECTION: no, in the analysis step, it seems that all.json does get written to! In particular, it is written to early on, right after unpickling and turning into a dataframe: 

                all_langs = random_langs + nauze_langs + dom_langs + evolutionary_path_langs # + natural_langs
                all_langs = unique_languages_by_vocab(all_langs, loading_bar=True)
                data = get_dataframe(all_langs)
                save_languages(all_langs, 'all.json')

            So all.json is actually probably adequate for some inspection.
            There's nothing spooky about the `get_dataframe` function; it seems to be simply measuring languages using pooling, and then mapping them into a dataframe.
            Unfortunately, due to randomness I can't inspect the exact languages I looked at above, '37131' shows a different language now in all.json.

            what could have been bad was a section in analyze.py where I measure the pareto front again. I suspect I did this because it smooths the curve; but I've commented it out (as I remember doing back and forth many times earlier this year). 
                - update: this doesn't seem to have done anything except cause interpolate to throw an error complaining about div by zero.

            The other thing could be the `rename_author_columns` function, which is super ugly and I'm willing to give it up since I'm not going to use this code to produce official figures anymore.

        - actually, the error I got was this:

            Saving languages ...
            0%|                                                                                                        | 0/72 [00:00<?, ?it/s]
            Traceback (most recent call last):
            File "/Users/nathanielimel/clms/thesis/code/modals/src/analysis/analyze.py", line 90, in <module>
                main()
            File "/Users/nathanielimel/clms/thesis/code/modals/src/analysis/analyze.py", line 57, in main
                save_languages(dominating_languages, 'dominant.json') # idk why I was indexing all_langs
            File "/Users/nathanielimel/clms/thesis/code/modals/src/file_util.py", line 92, in save_languages
                attributes["meanings"] = {i:{np.array2string(np.array(item_dict["arr"])): item_dict["string"]} for i, item_dict in enumerate(L.lexicon_)}
            AttributeError

        I recently changed the following line

            save_languages([all_langs[i] for i in dominating_indices], 'dominant.json') # overwrite dominant
        to 

            save_languages([all_langs[i] for i in dominating_indices], 'dominant.json') # overwrite dominant

        because I immediately above had

            dominating_languages = [languages[i] for i in dominating_indices]

        But it looks like I shouldn't have lol. Because here's where `languages` gets initialized:

            languages = pareto_raw_df.values.tolist()

        e.g, it's just a list of languages _as their pareto data_, instead of as language objects.

    When this code finally finished running, the results -- report and plot-- look exactly as before. This surprised me, because I uncommented the scaling complexity line, so I would have thought simplicity would have been changed. But complexity and simplicity are perfect -1 relationship, and the correlations are the same. Looking at the dataframe for all languages, I see that simplicity is no longer negative, but we have basically the same languages:

            complexity  comm_cost  simplicity  informativeness
        62       0.011364    0.93056    0.988636          0.06944
        36150    0.022727    0.93056    0.977273          0.06944
        36101    0.227273    0.92708    0.772727          0.07292
        28       0.113636    0.92708    0.886364          0.07292
        39670    0.181818    0.92667    0.818182          0.07333

    Without scaling the data, and commenting out the plot and df lines that rename the author columns, I get:

            complexity  informativeness  nauze
        62              1          0.06944    0.0
        55              2          0.11111    1.0
        8               2          0.12500    1.0
        36150           2          0.06944    0.0
        1994            3          0.10764    0.5
        ...           ...              ...    ...
        31916          84          0.12896    0.0
        16700          86          0.16835    0.2
        32748          86          0.13764    0.1
        32495          86          0.16305    0.1
        17601          88          0.14736    0.1
    
    Which all seems fine. At this point I may focus on more explicitly engineered debugging investigation.


7.3.22: print statements not showing up in evolutionary algorithm script
status: resolved*

    This is also an old bug that I forgot about, and noticed because I want output telling me when the estimation of the frontier is starting and the sampling has stopped.

    I have the following print statements

    print("Estimating pareto frontier ...")

    print("Sampling seed generation...")

    In my estimate_pareto_frontier.py, but they're not showing up in my system.output.txt file. Come to think of it, I actually don't see the typical 

    "Saved N sampled languages" printed from my save_languages util function. 

    Weird: I actually do see these print statements, but not until after the long thing I wanted to be warned was starting, finished, which is not useful. To clarify: I get all the print statements I put in, but not at the time I expected them.

7.3.22: Negative simplicity values in the old codebase:
status: resolved

    This is probably because I had the following line commented out:

        data = scale_complexity(data, max(list(data["complexity"])))

    And without this, in the function `add_columns`, simplicity is defined as 1 - complexity, but if complexity isn't in [0,1], we'll get negative values.

7.3.22: At this point, I will try to find the truth about the complexity correlations with naturalness by building something that can take the languages of the old codebase and turn them into equivalent altk / new codebase languages.
status: resolved

    The motivation is the following: I've checked the measures of naturalness (SAV / Nauze), and the complexity values of languages look good upon inspection of the old codebase. So we have (implicitly) two different datasets: languages with their complexity and naturalness data. These define 2 correlation coefficients. The task is to make sure these datasets are truly comparable, and then find the right coefficient.

    1. Implement tools to convert between old languages and the new ones.
    2. Run the complexity measures again on all the languages.
    3. Get the pearson correlations.

7.4.22: Writing the debug_correlations.py script
status: resolved

    So far this is a promising strategy. I load up the old languages as all.json, and then construct the analogous ModalLanguages. Along the way I keep track of any discrepancies between complexity in the old and new language, and between the old and new measure of naturalness. So far, no discrepancies for naturalness. I did get, however:

    With negation off in old but on in new:

        total discrepancies:  58251
        total old languages:  66321
        Saved 66321 old languages

    Upon closer inspection, though, I realized this is probably because I'm comparing two differnt complexity measures: one with negation, and one without. So next I will measure will negation turned off in the new codebase, and then run the old code with negation on and measure that too.

        - Trying this, I noticed that the new langs still had negation. So i'm looking into the modal_language_of_thought module for errors not following the configs and such.
            - Changed the line
                self.contains_negation = "negation" in lot_configs
            to 
            self.contains_negation = lot_configs["negation"]

    With negation off in both old and new:

        total discrepancies:  0
        total old languages:  66321
        Saved 66321 old languages

    Furthermore, here was the correlation data of naturalness with complexity, which is indeed quite high:

            sample  pearson
            new     -0.4685788796530204
            old     -0.4685788796530204
    
    Given this, I want to measure the tradeoff and do analysis in the new codebase with these converted old languages. My best guess is that something has happened with sampling so that the old languages represent a better sampling of the space, or at least one that results in more positive results.
    That is: 
    
        - I run the pipeline of the full experiment, but instead of sampling and using the evolutionary algorithm, I simply let the pool of languages consist of the old languages converted to ModalLanguages. After all, those languages have been found by exploration with the evolutionary algorithm and sampling from the old codebase.

    One thing I am noticing is that all the languages are being kept even after running set() on them to unique, but I doubt they really are all unique. I'm not sure yet why, because the expression names should just be the same as all expressions used in the new codebase. Looking at the hash function for ModalLanguage, I see as expected:

        # requiring diff name is a strong requirement
        return hash(tuple(sorted(self.expressions)))

    which ought to be uniquing things. But, in analysis, I still have the option of dropping duplicates, so I can probably be confident about not counting extras there.

    Here were the REPRODUCED results, which concur with the old codebase!

        Degree sav pearson correlations:
        simplicity: 0.46870328674046224
        complexity: -0.4687032867404632
        informativity: 0.492507973867846
        optimality: 0.5571371626617981

        MEANS
                        simplicity  complexity  informativity  optimality
        name                                                               
        natural_means       0.813636   16.400000       0.746296    0.957112
        dlsav_means         0.813636   16.400000       0.746296    0.957112
        population_means    0.634684   32.147834       0.532526    0.783940

        TTEST STATS
        natural languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic          9.842396   -9.842396       3.022135    7.417872
        Two-sided p-value    0.000598    0.000598       0.039081    0.001763

        dlsav languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic          9.842396   -9.842396       3.022135    7.417872
        Two-sided p-value    0.000598    0.000598       0.039081    0.001763

    The plot does look like more of the space is explored (in particular, the high informativity langs look more explored). But it's not dramatically different from the new plots i'm used to looking at at this point. I note that the informativity looks right, unlike in the old plot -- which had values past the prior. In fact, the similarity is so striking that I've 
    saved the two plots in a folder called `docs/first_correlation_debug_run`.

    This is without dropping duplicates, and the length of all_data.csv is 66325 without 5 the natural languages.
    These results can be found on the commit message "first correlation run".

    When I drop duplicates. The results don't change much. The all_data.csv has about half as many entries, 35170 + 5.

        Degree sav pearson correlations:
        simplicity: 0.4401090069250126
        complexity: -0.44010900692501265
        informativity: 0.6399118603515791
        optimality: 0.5432094201556613

        MEANS
                        simplicity  complexity  informativity  optimality
        name                                                               
        natural_means       0.813636   16.400000       0.746296    0.957112
        dlsav_means         0.813636   16.400000       0.746296    0.957112
        population_means    0.516987   42.505159       0.530802    0.708255

        TTEST STATS
        natural languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         16.315724  -16.315724       3.046501   10.659846
        Two-sided p-value    0.000083    0.000083       0.038159    0.000439

        dlsav languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         16.315724  -16.315724       3.046501   10.659846
        Two-sided p-value    0.000083    0.000083       0.038159    0.000439    

    - Next, I plan to run the same steps, but instead of keeping the new codebase sampled languages out of the analysis, I will let the pool of languages consist of all the old languages plus the new codebase sampled and explored ones.

    Here were the results:

        Degree sav pearson correlations:
        simplicity: 0.4151781617085524
        complexity: -0.4151781617085524
        informativity: 0.6770489229218878
        optimality: 0.5256428747463577

        MEANS
                        simplicity  complexity  informativity  optimality
        name                                                               
        natural_means       0.804545   17.200000       0.746296    0.955460
        dlsav_means         0.804545   17.200000       0.746296    0.955460
        population_means    0.517221   42.484588       0.533966    0.706636

        TTEST STATS
        natural languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         13.632536  -13.632536       3.001778   10.882309
        Two-sided p-value    0.000168    0.000168       0.039872    0.000405

        dlsav languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         13.632536  -13.632536       3.001778   10.882309
        Two-sided p-value    0.000168    0.000168       0.039872    0.000405

    You can view the results on the commit message "pooled old and new langs together in one experiment".

    For sanity, I'm also running the new code with negation turned off only, and no langs added from the old codebase. I've done this many times already, but I just want to check that the correlations are still quite low.
        - Interestingly, doing this resulted in an error in analysis.py in the call:
           data = get_dataframe(langs, **kwargs)
           .
           .
           .
            ValueError: Length of values (2528) does not match length of index (2529)

        Looking at the altk analysis submodule, I see

            # drop but count duplicates
            elif duplicates == "count":
                vcs = data.value_counts(subset=subset)
                data = data.drop_duplicates(subset=subset)
                data = data.sort_values(by=subset)
                data["counts"] = vcs.values

        I checked this logic on small toy data in a python environment, but it seemed correct.

        - RESOLVED: I looked at the data and there were many Nones and NaNs. I realized this was because I was loading languages that weren't measured: this was because I forgot to uncomment out the save_languages lines at the end of measure_tradeoff (which I had replaced with different saving lines for the above experiment combining old and new)!

    And sure enough, here are the very low correlations.

        Degree sav pearson correlations:
        simplicity: 0.0805049152987793
        complexity: -0.08050491529877928
        informativity: 0.7342289479023422
        optimality: 0.3294735688942782

        MEANS
                        simplicity  complexity  informativity  optimality
        name                                                               
        natural_means       0.800000   17.200000       0.746296    0.958545
        dlsav_means         0.800000   17.200000       0.746296    0.958545
        population_means    0.638135   31.120363       0.580944    0.807325

        TTEST STATS
        natural languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic          7.505357   -7.505357       2.337639    6.176596
        Two-sided p-value    0.001686    0.001686       0.079584    0.003490

        dlsav languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic          7.505357   -7.505357       2.337639    6.176596
        Two-sided p-value    0.001686    0.001686       0.079584    0.003490    


    Though this is _much_ higher correlation with informativity than I usually see. Don't know what could be going on here. Could be the lack of negation, but that doesn't make much sense because that should only impact complexity.


    Now I will run BOTH full experiments with negation turned on. This involves the following steps:
        - running the old code (less than 25 mins)
        - obtaining the json files lexicon.json, all.json
        - running the debug_correlations.py script and checking all the measurements are the same
        - deleting any languages saved in output folders
        - running the measure_tradeoff and analysis scripts as one check (just old langs)
        - running the entire pipeline, as a separate check (pooling the old and new langs)

    No discrepancies were found between the old and new language measurements:

        total discrepancies:  0
        total old languages:  66456
        Saved 66456 old languages

        sample  pearson
        new     -0.4091872227754934
        old     -0.4091872227754934

    Check 1: The old languages alone

    In the final analysis, I see that the correlation is high, but not as high as the debug script predicted (0.38 vs 0.40), which is strange, but could be due to dropping duplicates in the analysis script (indeed, looking at all_data, its only about 30k long instead of 66k).

        Degree sav pearson correlations:
        simplicity: 0.38204573364941424
        complexity: -0.38204573364941447
        informativity: 0.6553656982496618
        optimality: 0.5072446445166654

        MEANS
                        simplicity  complexity  informativity  optimality
        name                                                               
        natural_means       0.789744   16.400000       0.746296    0.955218
        dlsav_means         0.789744   16.400000       0.746296    0.955218
        population_means    0.556643   34.581826       0.531503    0.737487

        TTEST STATS
        natural languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         11.363641  -11.363641       3.036599    7.871290
        Two-sided p-value    0.000342    0.000342       0.038530    0.001408

        dlsav languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         11.363641  -11.363641       3.036599    7.871290
        Two-sided p-value    0.000342    0.000342       0.038530    0.001408 

    Check 2: the old langs combined with the new codebases pool of langs

    Here were the results: slightly lower complexity as expected,

        Degree sav pearson correlations:
        simplicity: 0.3568978222203001
        complexity: -0.3568978222203005
        informativity: 0.6823818567868
        optimality: 0.494225008839281

        MEANS
                        simplicity  complexity  informativity  optimality
        name                                                               
        natural_means       0.789744   16.400000       0.746296    0.953079
        dlsav_means         0.789744   16.400000       0.746296    0.953079
        population_means    0.556636   34.582355       0.533804    0.737567

        TTEST STATS
        natural languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         11.363972  -11.363972       3.004070    7.912685
        Two-sided p-value    0.000342    0.000342       0.039782    0.001380

        dlsav languages against population
                        simplicity  complexity  informativity  optimality
        stat                                                                
        t-statistic         11.363972  -11.363972       3.004070    7.912685
        Two-sided p-value    0.000342    0.000342       0.039782    0.001380

    Nothing too dramatic in the plot, other than the high informativity space looks better.

7.4.22: Taking samples of JUST 100% natural languages.
status: resolved

    I notice that in the old codebase this is something I did. I think this is a nice strategy. It raises the question of whether the sample is biased, and thus affecting correlations. In general, we really don't know how to take a good sample of the mathematically possible languages. 
        - Uniform sampling is really bad at finding optimal languages. 
        - And the evolutionary algorithm should be good at finding those optimal ones, but there may be some lock-in involved -- though we do randomly mutate the parents and save them to the explored langs, regardless of their local fitness, so I'm not sure why this is such a problem.

    Doing this in the new codebase:

        - Unfortunately, while passing in only the natural expressions lets the generate_languages() function run just fine, it doesn't seem to be finding any languages that the normal random sampler can't find. I know this because it's saving the same number before and after.
        In particular, we're back to almost 0 correlation again:
                    
            Degree sav pearson correlations:
            simplicity: -0.030188734478632562
            complexity: 0.030188734478632653
            informativity: 0.7449740074285384
            optimality: 0.26938205710854785        

        - HOWEVER! I turned up the knob on sample size from 2k to 10k, and got

            Degree sav pearson correlations:
            simplicity: 0.23183545729787725
            complexity: -0.23183545729787725
            informativity: 0.7811426304586019
            optimality: 0.46861543248553206        

        Which is far higher. This motivates turning up the sample size even higher, and also thinking about seeding the evolutionary algorithm with more degree 1.0 universal_property languages.
        
        The commit name is "Correlation with simplicity went way up as a result of sampling fully SAV languages and increasing sample size", which, lmao, sounds like p-hacking if I ever heard of it xD. But seriously, I'm not convinced that it is. P-hacking I think describes when you get a really unrepresentative sample, and the whole point is that we're exploring as best we can, possibly even better now than before. Whether there are more universal_property languages elsewhere in the space that hasn't been explored is still an open question. The _trend_ looks like the answer would be no, but at this point I'm cautious to make strong claims.

        Turning up to 20k sampling size, we get 15764 sampled languages and 20494 total langs. And the correlation with both simplicity and informativity goes up a bit:

            Degree sav pearson correlations:
            simplicity: 0.27448407524007945
            complexity: -0.2744840752400772
            informativity: 0.8034243588109712
            optimality: 0.5171836442749919

        This is for half_credit_literal. 
        Trying for indicator_literal with a sample size of 20k, we get 15764 sampled languages (which should be the same) and 20494 total langs (which could in principle be different, but not super worrying):

            Degree sav pearson correlations:
            simplicity: 0.27448407524007945
            complexity: -0.2744840752400772
            informativity: 0.8034243588109712
            optimality: 0.5171836442749919


    Adding degree 1.0 naturalness languages to the seed population of the evolutionary algorithm:

        - I actually noticed that the generate_languages function call for the seed actually starts off with _no_ criterion applied. I might change this to include the criterion to get even amounts of degrees, but since I'm going to inject with high naturalness languages anyway, I'll do the latter first.

        - the results were about the same, with slightly less languages explored during the evolutionary algorithm. That makes some sense: I gave it a head start to the extent highly natural languages are closer to the frontier.

            19634 total langs.
            .
            .
            .
            Degree sav pearson correlations:
            simplicity: 0.27942214075127864
            complexity: -0.27942214075127825
            informativity: 0.8005810203039387
            optimality: 0.5191003974244972

7.4.22: Testing with very large sampling size
status: resolved

    Given that I can increase the optimality correlation with naturalness by sampling more degree 1.0 languages, a natural next step which addresses both sampling more optimal languages and sampling more of the space in general, is simply to ask for a very large sample size.

    In particular, I won't inject degree 1.0 langs in sampling. There will be just one call to `generate_languages`. 

        - First, I will try 20k.

        Results for half_credit_literal look like they confirm the hypothesis: 

            18529 total langs.

            Degree sav pearson correlations:
            simplicity: 0.27478177250674074
            complexity: -0.2747817725067404
            informativity: 0.8070911122098514
            optimality: 0.5358468342428402

        Results for indicator_literal similarly (and are more positive):

            18640 total langs.

            Degree sav pearson correlations:
            simplicity: 0.29572958275332895
            complexity: -0.29572958275332895
            informativity: 0.7995081848160996
            optimality: 0.460000315343279

        And this was 15764 sampled languages, as before with injecting the degree1 langs.

        - Increasing the sample size to 40k:

            Notably, this results in 30710 sampled languages. Interestingly, the evolutionary algorithm doesn't do much to explore: it finds 33363 explored languages, and the final analysis has just 33368 total langs (add 5 for the naturals). 
                - This means that much of the work can be done by the sampling script, and not the evolutionary algorithm -- which is opposite of what I had been believing. Also, the sampling script is faster at finding languages (though this could partly be due to the fact the evolutionary algorithm measures each generation, and there's no measurement in the sampler.)
                - I also note that (after dropping duplicates) the all_data.csv has about 23k entries.

            I also started timing the full experiment in run.sh. The 40k run took 8m17.086s total. (half_credit_literal)

            Here were the results:

                Degree sav pearson correlations:
                simplicity: 0.30819300359460555
                complexity: -0.30819300359460544
                informativity: 0.8317336287804182
                optimality: 0.5668854005095101

            Again, the plot shows that the low comm_cost space is still very unexplored. I'm also going to stop printing complexity correlations until I need them again.

        IMPORTANT: Unfortunately, I forgot to remove the injecting of degree1 langs into the evolutionary sampler. The generation size is the sample size, which remained small (2k) and as noted previously, at this size does not have an effect, but I need to remove this extra call to `generate_languages` and run again.

            - Fortunately, I've confirmed that the above results do in fact hold when you don't inject the seed languages with degree1 langs. It's actually not that big of a deal to me to do this actually, because it's a bit like giving the evoluationary algorithm a prior or headstart, but it is objectively exploring fitness independent of its seed after X generations.
            These were the results (though they are after changing the precondition for AddPoint to be less restrictive)

                Degree sav pearson correlations:
                simplicity: 0.32594052876431134
                informativity: 0.8223497501162024
                optimality: 0.6001674393381625


        Patas run on all configs with 60k sample size -- without the help of new AddPoint:

            - indicator_literal

                48533 total langs.

                simplicity: 0.35324353074601395
                informativity: 0.8075340747083615
                optimality: 0.4956028725636091

                I do note that the plot looks pretty disappointing, very low exploration of high informativity space.

            - indicator_pragmatic
                48686 total langs.

                Degree sav pearson correlations:
                simplicity: 0.3565456977614203
                informativity: 0.7592295348947574
                optimality: 0.5619919946361154

                As usual, this plot has many perfect informativity languages, though we know that this is expected and does not mean that many literally perfect informativity languages exist. This is a good illustration of why it's important to make sure the literal informativity experiments are robust first, because we will be measuring the same languages in both cases, but pragmatics in some sense obscures the shortcomings of bad sampling.

            - half_credit_literal

                48432 total langs.

                simplicity: 0.3291947721105495
                informativity: 0.84123551942777
                optimality: 0.6028356910410583

            - half_credit_pragmatic

                48477 total langs.

                simplicity: 0.31439208466348345
                informativity: 0.7049393865716511
                optimality: 0.38530611462020353

        These changes won't be committed because I don't want to deal with the large file complaint. I'll also note that this took 01:38:21, and I think I might even have better luck on my local. But at least it runs to completion, and if I ever want something to run and close my laptop, patas is a good option.

        One thing I do note is that I have also removed the sanity check and synonymy languages in the seeding of the evolutionary algorithm, and this still results in the low comm_cost space not being explored. Given that I can get the same results with this higher sampling number, my hypothesis is that the low comm_cost is not well explored by uniform sampling. It may have been well explored by the evolutionary algorithm at some point, but I don't know what has gone wrong. This inspires a very important conjecture:
            - The reason why I'm getting like 0.3 correlation at the highest right now, instead of the old codebase's 0.4Xs, could be because the low comm_cost space is not being explored enough. So my next hypothesis to test is:
                - Find a way to increase the high informativity space sample, because this may result in higher correlation of naturalness with optimality.

7.5.22: Investigating informativity promoting mutations in evolutionary algorithm.
status: resolved

    A basic response to why perfectly informative languages are not being discovered is that: they're not allowed to for some reason, lol.

    More precisely, the way that languages become more fit is through mutation. At each generation, the most fit languages are mutated. But if languages are not allowed to become more informative, then this could explain why the space isn't getting explored. In particular, there could be a check in AddPoint that requires the informativity to not be close to 1.0. 

        - Just immediately, this will prevent high complexity, perfect informativity languages!! This explains why there's nothing inbetween the sanity check and synonym langs.
        - Moreover, it may be too strong a requirement for informativity to be `np.is_close`. Inspecting some of the dominant languges, some of the comm_cost values are quite low: SanityCheck descendants are near zero, which means they can't have high complexity children. However, inspecting the dominant languages for more recent runs without the sanity checks, the lowest comm_cost is the prior, at 0.1667, which is not close to 0.
            >>> np.isclose(0.0, 0.1667)
            False
            Although, looking at a SanityCheck language, the same holds:
            >>> np.isclose(0.0, -0.0024568085877116896)
            False

            So in principle this hypothesis may be false, since the AddPoint mutation should be able to still apply, but I think the concerns are real enough to remove this condition on not being informativity 1.0 entirely.

        - Upon inspection, I wasn't comparing informativity directly, but only uncovered points. this was the relevant code:

            def precondition(self, language: ModalLanguage, **kwargs) -> bool:
                """Only apply when every point is not already expressible."""
                # Check if any inexpressible meanings
                return bool(uncovered_points(language))
        
        I changed this to 

                return True
        
        and then upon realizing the problem with lang_size stuff, I just made AddPoint inherit from AddModal, and call its super for the precondition.

        And in the mutation method, I changed what's in the if block to:

        
            point = random.choice(language.universe.objects)
            if uncovered_points(language):
                point = random.choice(list(uncovered_points(language)))
        

        The results were slightly better on the statistics side, and also looking at the plot, the high informativity languages _do_ look more discovered. However, the effect is not dramatic, and still no perfect languages are discovered, which is surprising!

        Next I want to inspect the other mutations. I'm not currently calling RemovePoint, but it does call upon informativity, because it's supposed to increase it. It has this precondition: 

            def precondition(self, language: ModalLanguage, **kwargs) -> bool:
                """Only apply when language is not perfectly informative, or when it has a modal that expresses more than one meaning point."""

                # Is not already perfectly informative
                inf = kwargs["objectives"]["informativity"](language)
                if inf is None or np.isclose(inf, 1.0):
                    return False

                # Can express more than one point
                expressions = language.expressions
                for expression in expressions:
                    points = expression.meaning.objects
                    if len(points) > 1:
                        return True
                return False

        Though I don't recall this mutation being helpful in this codebase, I will add it to the list of mutations, and actually remove the first condition, but keep the second.

        I do note however that it was being used in the old codebase. in fact, here is a list of the mutations used in the old codebase:
            - add
            - remove
            - interchange
            - remove_bit

            I note that the implementation of remove_bit is significantly different from my RemovePoint. In remove_bit, I just randomly select an expression and remove a point from its meaning space, and then as long as the resulting expression doesn't have 0 points in its meaning, I replace the modal with this less ambiguous one.

                # randomly select a modal
                for index in range(language.size()):
                    lex = language.lexicon_.copy()
                    arr = np.array(lex[index]["arr"])

                    if np.count_nonzero(arr) == 0:
                        raise ValueError('Array to replace in remove_bit cannot be all 0.')

                    # randomly remove a bit
                    argw = list(np.argwhere(arr))
                    if len(argw) == 0:
                        continue
                    loc = random.choice(argw)
                    row, col = loc
                    arr[row, col] = 0
                    if np.count_nonzero(arr) != 0:
                        key = np.array2string(arr)
                        # replace the modal
                        new_expression = expressions[key]
                        language.pop(index)
                        language.add_modal(new_expression)
                        break
                return language


        Looking at RemovePoint, I think I may have forgotten not to merely add the new modal, but to REPLACE the old ambiguous one!

                # Search for the correct expression
                for e in expressions:
                    points_ = e.meaning.objects
                    if set(points) == set(points_):
                        new_expression = e
                        break
        >>>>> here I should have deleted the old expression! >>>>>
                language.add_expression(new_expression)
    >>>>But?    language.pop(index)
                return language        

        actually, the problem is more subtle. language.pop(index) actually _should_ be taking care of the replace logic -- but it's doing something worse: its the last index reached by the enumerate(vocab) loop. This means that if we are handed a perfectly informative language, this function will REDUCE ITS INFORMATIVITY because it will still pop the last expression!!

        Ah, after fixing this the stats still aren't in the .40 range. On the bright side, the language size seems to have increased a bit, indicating that more langs are getting explored.

            39042 total langs.

            Degree sav pearson correlations:
            simplicity: 0.31006584292647443
            informativity: 0.813443407291671
            optimality: 0.5404819315463313

        The plot looks about as good as below. It really looks like there are three languages with perfect informativity. When I look in artificial.yml, I find perfectly informative languages! Restate: The perfect informativity space is now being explored with some success: specifically, the following search turns up 9 languages:

              informativity: 0.9999999999999999

        And in dominant.yml, I found the perfect informativity language (the one with six expressions all denoting one point):

            - sampled_lang_444874:
                data:
                Language: artificial
                comm_cost: 1.1102230246251565e-16
                complexity: 24
                dlsav: true
                iff: 1.0
                informativity: 0.9999999999999999
                name: sampled_lang_444874
                optimality: 0.9999999999999999
                sav: 1.0
                simplicity: null
                expressions:
                - form: dummy_form_15
                lot: (* (weak ) (deontic ))
                meaning:
                - weak+deontic
                - form: dummy_form_0
                lot: (* (strong ) (circumstantial ))
                meaning:
                - strong+circumstantial
                - form: dummy_form_1
                lot: (* (strong ) (deontic ))
                meaning:
                - strong+deontic
                - form: dummy_form_31
                lot: (* (weak ) (epistemic ))
                meaning:
                - weak+epistemic
                - form: dummy_form_3
                lot: (* (strong ) (epistemic ))
                meaning:
                - strong+epistemic
                - form: dummy_form_7
                lot: (* (weak ) (circumstantial ))
                meaning:
                - weak+circumstantial        


    This is enough progress to close up this entry for now. The upshot: I am successfully finding the perfectly informative languages in my evoluationary algorithm, most likely as a result of debugging some of the logic errors in my mutations AddPoint and RemovePoint. Partly because my correlation of naturalness with simplicity is _STILL_ not as high as in the old codebase, though, I'm somewhat skeptical that the work is done. I'm at around 0.31, but my old code was finding results yielding around 0.41 - 0.47.
    My hypothesis is that the high informativity space is being explored better, but not as well as in the old codebase. Another possibility is that the old codebase is better at exploring low-complexity languages in general.
    Ideas to consider: increasing the sample size (which I know helps somewhat) and engineering mutations that reduce complexity (which I haven't thought of yet).

7.5.22: Four corner optimization as a sampling method
status: resolved* (works but is slow, and have only done it twice so far)

    What if I just ran the evoluationary algorithm but instead of optimizing for complexity and comm_cost, i optimize for complexity and informativeness?
    (and add this to the first result)

        - The first time I tried doing this, nothing changed at all -- not even the languages found.

    And for that matter, why don't I just run the evolutionary algorithm 4 times, once for each of:

        1. (comm_cost, complexity) # lower left, the actual efficiency tradeoff
        2. (informativity, complexity) # lower right
        3. (informativity, simplicity) # upper right
        4. (comm_cost, simplicity) # upper left

    I'd be especially interested in the 2nd and 4th combinations, because these are the areas so far quite unexplored. In fact, the upper left has to my mind _never_ been explored for as long as I've been working on this research.        

    A question that arises is how to measure simplicity. 
    I think the following two options could both work, since complexity is always nonzero (and in particular is minimized at 1.0):
         1 / complexity
         -1 * complexity

    I note that the first time I ran this in a full experiment, the first two objectives finished in under 2 minutes. However, the third took more than 8 minutes. I didn't measure the fourth one in this experiment. I think this has proven extremely useful for sampling. It explores almost 1.2 million languages:

        # evolutionary algorithm results
        Discovered 1194189 languages.
        Saved 66038 explored languages
        Saved 21 dominant languages

        # some langs get dropped in measure_tradeoff, idk why
        65992 total langs.
        Saved 65992 sampled languages
        Saved 17 dominant languages
        Saved 5 natural languages

    What's interesting is that this is actually quite close to the amount of languages usually generated by the old codebase. I'll be surprised if the plot doesn't look different this time.

    In total, this took 23m48.351s. Note also that the correlations are lower:

        Degree sav pearson correlations:
        simplicity: 0.21273099285552405
        informativity: 0.8150446212912054
        optimality: 0.5099936368543856
    
    This is an interesting result. However, the plot _STILL_ doesn't look dramatically different, which is disappointing.
        - I do think that some of this could definitely be due to the nature of the mutations allowed. We've already seen that optimization results are extremely sensitive to the mutations applied and when they're allowed to apply.

    Though I've now set up the measure_tradeoff script to reset the simplicity values of each language to be None. I'm going to commit, and re-run, this time with all 4 corners.

    Well, it took an hour, but I've finally reproduced the results from the old codebase, and I'm very confident in the results. Here they are:


        173250 total langs.
        Measuring min distance to frontier ...
        Setting optimality ...
        Saved 173250 sampled languages
        Saved 19 dominant languages
        Saved 5 natural languages

        Degree sav pearson correlations:
        simplicity: 0.48401273629041136
        informativity: 0.7480614473861689
        optimality: 0.6289970536861712
    

        MEANS
                        simplicity  informativity  optimality
        name                                                   
        natural_means       0.795000       0.746296    0.952014
        dlsav_means         0.795000       0.746296    0.952014
        population_means    0.424771       0.533981    0.659169

        TTEST STATS
        natural languages against population
                        simplicity  informativity  optimality
        stat                                                    
        t-statistic          18.51147       3.001566   11.300500
        Two-sided p-value     0.00005       0.039880    0.000349

        dlsav languages against population
                        simplicity  informativity  optimality
        stat                                                    
        t-statistic          18.51147       3.001566   11.300500
        Two-sided p-value     0.00005       0.039880    0.000349


        real    68m53.951s
        user    53m57.331s
        sys     4m28.764s

    And the plot looks _much_ more explored. The perfect and high informativity space looks thoroughly explored. The other corners are more explored than they every have been. I'm still getting not that many DLSAV languages in the low informativity, low complexity area, but that could be because that's truly where they're more sparse. After all, we knew that informativity was doing most of the work.

    *************CELEBRATION*********TADA***********YAY***************

    I think this is a good sampling methodology, it samples not just the optimal languages, and it does so in a principled and mostly unbiased way. One way to improve it would be:
        - speedup
        - to add more specific mutations for the different objectives.

    Also, Unfortunately but not a surprise: github rejects my artificial.yml file.
    Another downside: this takes longer than an hour to run.

    To conclude: 
        - I got the statistics I was looking for, namely those more congruent with the old codebase correlations with complexity, around 0.48. I'm also more confident in these stats than any others i've measured, because the method seems less biased to me. The sample size is also far higher: 173k langs.
        - The plot looks more explored.
        - some TODOs left are speed, and better mutations for each corner objective.
        - figure out what to do about the fact github can't handle my files now.

7.5.22: FALSE ALARM: Don't forget your complexity measure doubles the atoms that are not TOP and BOTTOM.
status: resolved

    For a year I've been seeing lang complexity up at like 70 or 80. But the maximum language complexity is 40. Why is this? Because we double the atoms that are not (1).

    SO, correction: the maximum language complexity is 80, not 40.
    Explictly:


        complexity = (2 * num_atoms) + (1 * occurences of TOP)
    
    And note that the doubling term always dominates.

    for a fixed language size N, the maximum language complexity is just 2 * num_atoms. So if you ever write a test, you can check that the value is correct and is even.


7.6.22: Preparing the 4-corner result as the main results for SALT

    - After meeting with Shane, we decide that we will likely use the results of the four-corner optimization /exploration for the main results of our SALT paper. 
    - Some changes that needed to apply to the plot: 
        - flip axes, 
            - Ran into an issue here where complexity and comm_cost values were not swapped: that is, complexity ranged from 0 to 1, but comm_cost ranged from 1 to 80.
                - This was just because I forgot to flip the xlab and ylab labels.
        - use classic theme,
        - record the counts for size.
            - Recording counts as size makes the DLSAV key harder to see, because before it was DLSAV as size. There may be some other shapes we can use instead, but I don't see this as a major issue.
            - This raised an issue where the max simplicity, high comm_cost point has one gigantic dot of 2500ish langs, but based on the discussion with Shane we expected there to be just 6 languages. Moving this to its own issue now.

    - A change to the statistics: don't drop duplicates. Just for the sake of being explicit, I'll put some of the discussion I had with shane today below:

        Regarding the shape of the resulting explored plot:
            - We are more confident than ever now that we have explored the space well. The sparse areas of the plot may in fact be low density regions of the language space. What does this mean?
                - This means that there is NOT a bijection from languages <-> points. Every language corresponds to some point. But the same point may be the measurement values of MANY languages. And it is not the case that the set of points (the range of the function) is covered by the input space.
                - This is a natural consequence of one main fact: the language space is discrete, while the space of points is continuous. 
                - There is also the matter of the transformation / mapping: a reason you will never see a perfectly precise, but highly complex langauge being higher in complexity than lower informativity languages is because perfectly unambiguous synonyms will always get beat out by ambiguous, but very unnatural (and thus complex, e.g. think "mought"), expressions. This means that the upper left part of the space will never be explored.
                    - This is an important point. I will restate it: the fact that perfectly informative languages must sacrifice informativity to become more complex entails that max complexity, max informativity languages are impossible. 
                    - There may be some thinking here to do and even some research ideas. At the same time, it may be something that only holds for restricted domains (e.g. function instead of content words), because it seems to be a part of the tradeoff when you encode the possibility of synonymy / meaning overlap. 

    - An mild concern: I don't see the natural languages anymore. this may be because they were dropped by my constructing a copy of the final dataframe and doing the "counts" on that one, whereas I probably need to count first, and then add the natural languges. I haven't checked this, but I'm not concerned because this is an issue for after SALT.
    - To that end, I'm keeping languages in the data, but removing them from the plot. So we still benefit from discovering those languages and adding them to our pool, but we don't identify them as natural yet ;)

    Here are the results when you leave all observations in. Correlation with simplicity increased by about 0.03, and optimality increased by about 0.02. But correlation with optimality increased by more than 0.6.

        Degree sav pearson correlations:
        simplicity: 0.5124885250713449
        informativity: 0.7690636331914136
        optimality: 0.6896187431039775

        MEANS
                        simplicity  informativity  optimality
        name                                                   
        natural_means       0.795000       0.746296    0.952014
        dlsav_means         0.795000       0.746296    0.952014
        population_means    0.407141       0.516596    0.645423

        TTEST STATS
        natural languages against population
                        simplicity  informativity  optimality
        stat                                                    
        t-statistic         19.392966       3.247343   11.830951
        Two-sided p-value    0.000042       0.031455    0.000292

        dlsav languages against population
                        simplicity  informativity  optimality
        stat                                                    
        t-statistic         19.392966       3.247343   11.830951
        Two-sided p-value    0.000042       0.031455    0.000292

    - On saving the huge pool (173k) of langs: my solution for now is to create a local folder for each commit, and store the artificial.yml language there. May move to dropbox, or look into git large file storage, or just start adding the language YMLs to gitignore. Will have to think about it.

7.6.22: max simplicity, high comm_cost point has one gigantic dot of 2500ish langs
status: almost solved

    In the top left of Pareto frontier (on SALT version of plot with comm_cost on y axis and complexity on x axis), there is a giant point of 2500 languages. This may be due to a bug of recording more than the theoretically possible number of unique languages, which should just be one language for each meaning point, so 6 for a semantic space of (2,3).

    Hypothesis: I suspect this is the result of a rounding error. Because in the past, I've seen that comm_cost and informativity can take all kinds of floating point values that are probably equivalent.

    But my strategy was first to look at the data. Specifically, I loaded up the all_data.csv (which github allows me to track) and performed the "counts" logic in analysis on it, which yielded

                    complexity  comm_cost                  name  dlsav  counts
            37671            1   0.583333    sampled_lang_34030  False    2627
            23122            2   0.666667    sampled_lang_33973   True    1376
            37563            2   0.666667    sampled_lang_33995  False     855
            22855            2   0.750000    sampled_lang_33968   True     846
            23636            3   0.520833    sampled_lang_35857  False     702
            ...            ...        ...                   ...    ...     ...
            111964          80   0.696181  sampled_lang_1366049  False       1
            325             80   0.696627  sampled_lang_1369470  False       1
            1525            80   0.750000  sampled_lang_1571344  False       1
            239             80   0.752778  sampled_lang_1407894  False       1
            15362           80   0.833333  sampled_lang_1389258  False       1

    We see that there are 2627 languages with just complexity 1. This sounds like an error to me: there should only be one language with complexity 1, the language containing just TOP.

    Another very important point: when you start counting, the language "name" column becomes lossy or misleading. It is just the "name" of the first language with those measurements.

    The next step I will do is identify these languages which have complexity 1. 
    There was just one such language, and I confirmed this was the case when searching the yaml file by text too.
    So, something must have gone wrong from the time I loaded the languages and got their dataframe, or dropped duplicates from it.
        - I can try to use altk's get_dataframe function, and then check how many entries have complexity 1, to rule out altk as the culprit.
            - Indeed, when I do subset the resulting dataframe using
                `data[data["complexity"]==1]`
            I get a dataframe with just one language.

        - Next I want to see if my "counts" logic is the culprit, so I will run it and see if this produces the 2626 extra counts. There's good reason to think that it's actually distributing the counts incorrectly, too -- because maybe the languages which were counted as complexity 1 are actually complexity X languages, possible duplicates. In other words I expect once this is resolved, the counts will be more evenly distributed, at least along the frontier.
            - I found the culprit. value_counts() on a pd.DataFrame will by default sort the rows of the input dataframe by frequency. This is detailed in the docs. So we've actually created

                complexity  comm_cost   
                36          1.110223e-16    2627
                            1.666667e-01    1376
                40          1.110223e-16     855
                64          6.562500e-01     846
                72          6.574074e-01     702
                                            ... 
                30          1.898148e-01       1
                46          5.770833e-01       1
                30          1.898148e-01       1
                            1.875000e-01       1
                1           5.833333e-01       1

            But if we set sort=False, we get the expected result:

                complexity  comm_cost
                1           0.583333       1
                2           0.666667       2
                            0.666667       3
                            0.750000       3
                3           0.520833       1
                                        ... 
                80          0.696181       1
                            0.696627       1
                            0.750000     105
                            0.752778     100
                            0.833333      13
        
        If we look at the code that does the "counts" logic, we have

            plot_data = data.copy()
            vcs = plot_data.value_counts(subset=subset) <<<<<< This sorts the dataframe by frequency, so we need to specify sort=False
            plot_data = plot_data.drop_duplicates(subset=subset)
            plot_data = plot_data.sort_values(by=subset) <<<<<<< This sorts the dataframe according to ascending=True, which is correct
            plot_data["counts"] = vcs.values

        There are a few tricky issues with pandas. One is that value_counts DOES sort, even when you specify sort=False: it sorts by the subset you supplied. So you do need to call sort_values on the dataframe. 
        And I believe the following is what we want

            plot_data = data.copy()

            # get counts. Will return values sorted according to the subsets.
            vcs = plot_data.value_counts(subset=subset, sort=False)  

            # keeps the first occurence of the observation by default, 
            # so we have the right number of entries,
            # but we stil need to sort the same way as value_counts.
            plot_data = plot_data.drop_duplicates(subset=subset)
            
            # sort according to the subsets.
            plot_data = plot_data.sort_values(by=subset)

            # Now we've ensured that size and order match,
            # so assign these counts to the df
            plot_data["counts"] = vcs.values


        I'm now confident this will resolve the issue. Running the analysis script again to see the difference. (note that this won't affect the stats, since I just use a separate dataframe with all points and none of this logic involved).

        After running, the plot looked much like before, with the vast majority of points very small, and a handful with larger size.

        I note that the most frequent point, the one with 2627 counterparts, was one with the following measurements (which align with the plot's visualization):

            comm_cost: 1.1102230246251565e-16
            complexity: 36
            dlsav: true        
        
        That is, we get a large yellow triangle at the bottom of the plot. It is not an optimal langauge -- and it's easy to see why: even though it's perfectly informative, it has more expressions than it needs.

        Here is an example of one of the 2767 languages that have this measurement:

            (You can actually see the synonymy logic I built in above, in the fact that synonyms have unique forms (so that they can be indexed correctly in the sender and receiver matrices).)

            - sampled_lang_855467:
                data:
                Language: artificial
                comm_cost: 1.1102230246251565e-16
                complexity: 36
                dlsav: true
                iff: 1.0
                informativity: 0.9999999999999999
                name: sampled_lang_855467
                optimality: 0.8939339828220179
                sav: 1.0
                simplicity: null
                expressions:
                - form: dummy_form_3_1
                lot: (* (strong ) (epistemic ))
                meaning:
                - strong+epistemic
                - form: dummy_form_1_1 <<< notice the extra '_1' for uniqueness
                lot: (* (strong ) (deontic ))
                meaning:
                - strong+deontic
                - form: dummy_form_7_1
                lot: (* (weak ) (circumstantial ))
                meaning:
                - weak+circumstantial
                - form: dummy_form_0_1
                lot: (* (strong ) (circumstantial ))
                meaning:
                - strong+circumstantial
                - form: dummy_form_15_1
                lot: (* (weak ) (deontic ))
                meaning:
                - weak+deontic
                - form: dummy_form_31_1
                lot: (* (weak ) (epistemic ))
                meaning:
                - weak+epistemic
                - form: dummy_form_1
                lot: (* (strong ) (deontic )) <<<< already seen
                meaning:
                - strong+deontic
                - form: dummy_form_7_0
                lot: (* (weak ) (circumstantial )) <<<< already seen
                meaning:
                - weak+circumstantial
                - form: dummy_form_31
                lot: (* (weak ) (epistemic )) <<<< already seen
                meaning:
                - weak+epistemic        

        I would conjecture that every langauge with this measurement has 9 expressions and exactly three synonyms. 
            - I have confirmed this by loading them up and checking.
        
        One question that this raises, is that in terms of combinatorics, shouldn't the languages of size 10 yield more possible synonyms, and thus shouldn't the set of possible languages be larger, and thus the counts, and thus the size of the triangle for complexity 40 languages? That triangle is large, but not the largest.
            - These languages correspond to counterparts of the following:

                - sampled_lang_845898:
                    data:
                    Language: artificial
                    comm_cost: 1.1102230246251565e-16
                    complexity: 40
                    dlsav: true
                    iff: 1.0
                    informativity: 0.9999999999999999
                    name: sampled_lang_845898
                    optimality: 0.8585786437626906
                    sav: 1.0
                    simplicity: null
                    expressions:
                    - form: dummy_form_3_1
                    lot: (* (strong ) (epistemic ))
                    meaning:
                    - strong+epistemic
                    - form: dummy_form_1_1
                    lot: (* (strong ) (deontic ))
                    meaning:
                    - strong+deontic
                    - form: dummy_form_7
                    lot: (* (weak ) (circumstantial ))
                    meaning:
                    - weak+circumstantial
                    - form: dummy_form_0_1
                    lot: (* (strong ) (circumstantial ))
                    meaning:
                    - strong+circumstantial
                    - form: dummy_form_15_1
                    lot: (* (weak ) (deontic ))
                    meaning:
                    - weak+deontic
                    - form: dummy_form_31_1
                    lot: (* (weak ) (epistemic ))
                    meaning:
                    - weak+epistemic
                    - form: dummy_form_15_0
                    lot: (* (weak ) (deontic )) <<< synonym
                    meaning:
                    - weak+deontic
                    - form: dummy_form_31
                    lot: (* (weak ) (epistemic )) <<< synonym
                    meaning:
                    - weak+epistemic
                    - form: dummy_form_3_0
                    lot: (* (strong ) (epistemic )) <<<< synonym
                    meaning:
                    - strong+epistemic
                    - form: dummy_form_1
                    lot: (* (strong ) (deontic )) <<<< synonym
                    meaning:
                    - strong+deontic            

        It makes sense that there are only 4 possible synonyms -- any more than that and you lose informativity. 
        Now we can mathematically compare the 36 complexity langs to 40 complexity langs by asking the question: 

        NOTE: see https://www.andrew.cmu.edu/user/ps7z/prob.pdf for an quick and clear explanation of factorial, permuations, and combinations.

        The statement of the problem:

            - How many ways are there to choose k objects from a list of n items, allowing repetition of objects, and order doesn't matter?
                - https://math.stackexchange.com/questions/794716/the-number-of-different-ways-to-choose-k-out-of-n-unique-items
                - its \binom{k + n - 1}{k}

            - How many ways are there to choose 3 expressions (the synonyms, allowing repetition) from 6 possible expressions, where order doesn't matter?
                WRONG: 6c3 = 20
                CORRECT: n^k = 6^3 = 
                
            vs
            - How many ways are there to choose 4 expressions from 6 possible expressions with repetition without order?
                WRONG: 6c4 = 15
                
            
        So it is true that there are more possible 'small' languages in this sense. 
        
        But I'm still puzzled as to why there are as many as there are: why are there 2627 points?

            - Well, we know there are that many languages -- i've confirmed that in the artificial.yml file. 
            - I think the answer is straightforward: my hashing function is incorrect for languages. This is a big enough bug fix that I'm moving to a new issue entry.
            

7.6.22: The hash function for ModalLanguages is too strong, as a result of the logic used for allowing synonyms to have different hashes.
status: resolved

        My hashing function is incorrect for languages. Right now it's 

            return hash(tuple(sorted(self.expressions)))

        But we know that the hash for the expressions will return unique hashes for synonyms, because of the form logic that I built in. And we need to keep this, because we need it for the Sender/Receiver matrix operations. So a natural solution is to keep the hashing on the expression level, but when comparing at the language level, we abstract away from the form differences in synonyms: and only look at the hashes for expressions QUA meanings. 
        
        A first pass would be

            return hash(tuple(sorted([e.meaning for e in self.expressions])))

        Where the hash for a ModalMeaning (which I've just changed from not requiring sorting to requiring it) is now:

            return hash(tuple(sorted(self.objects)))

        This throws an error, though, because I haven't implemented the less than operator for meanings:

            Traceback (most recent call last):
            File "<stdin>", line 1, in <module>
            File "/Users/nathanielimel/clms/projects/modals-effcomm/src/misc/file_util.py", line 181, in load_languages
                set(
            File "/Users/nathanielimel/clms/projects/modals-effcomm/src/modals/modal_language.py", line 159, in __hash__
                return hash(tuple(sorted([e.meaning for e in self.expressions])))
            TypeError: '<' not supported between instances of 'ModalMeaning' and 'ModalMeaning'
        
        So I'll use the LOT strings, because even though that's just as arbitrary as implementing a less than for ModalMeaning, it's already done. 
        
        That is, the next try is:

            return hash(tuple(sorted([e.lot_expression for e in self.expressions])))

        This throws no errors. It also yields far fewer unique languages as expected. However, I expected (sic) 6 choose 3 = 20 languages, but I got 56.
            - NOTE: this math is wrong, but I'm keeping it here for posterity.

            len(unique_languages)
            107372
            >>> langs = [lang for lang in languages if lang.data["complexity"]==36 and lang.data["comm_cost"]==1.1102230246251565e-16]
            >>> len(langs)
            2627
            >>> langs_unique = [lang for lang in unique_languages if lang.data["complexity"]==36 and lang.data["comm_cost"]==1.1102230246251565e-16]
            >>> len(langs_unique)
            56

            - THIS IS THE CORRECT NUMBER of languages but I didn't know it. Here's why:
                - with repetition and without order significance 
                = \binom{k + n - 1}{k}
                with k = 3 and n = 6
                = 8 choose 3
                = 56
        
        So, I created a list of all the unique LoT expressions, for each of these languages, and checked out the first element:

            >>> vocabs = [list(set([e.lot_expression for e in lang.expressions])) for lang in langs_unique]        

            >>> for lot in vocabs[0]:
            ...     print(lot)
            ... 
            (* (weak ) (deontic ))
            (* (strong ) (deontic ))
            (* (strong ) (epistemic ))
            (* (weak ) (circumstantial ))
            (* (weak ) (epistemic ))
            (* (strong ) (circumstantial ))
        
        Which shows just the 6 unambiguous points as expected.

        The next step is to make sure that they're all the same set. The purpose of this check is to rule out if there are anything other than unambiguous points.
            - Indeed, there's just one set in vocabs.

        -----
        The length of our list of unique languages of point=(36, 1.1102230246251565e-16) is the number that was predicted by combinatorics if we correctly hashed.

        I will now check the list of unique languages of point=(40, 1.1102230246251565e-16) for the same checks:
            - the set of meanings is just the same 6 lot expressions
            - it has length = \binom{k + n - 1}{k}

        length of langs=113
        13 choose 4 = 715

            - So the length of these languages is not exactly the number of possible combinations. But that's okay -- we only require that it's not greater, and that it's not greater than the lang size 9, e.g. complexity 36 languages. This is because it can be hard to sample all possible combinations. It is promising enough that this list of languages is already double the list of 36 complexity languages.

        And I have confirmed that the only set of meanings is the 6 unambiguous ones. 

        This is sufficient for me to mark this issue resolved within the scope it was started for.

7.6.22: KeyError with dlsav
status: resolved

    When running the full experiment above, I ran into the following error during the analyze script:

        Traceback (most recent call last):
        File "/Users/nathanielimel/clms/projects/modals-effcomm/src/analyze.py", line 202, in <module>
            main()
        File "/Users/nathanielimel/clms/projects/modals-effcomm/src/analyze.py", line 115, in main
            natural_data = get_dataframe(nat_langs, **kwargs)
        File "/Users/nathanielimel/miniforge3/envs/modals-effcomm/lib/python3.10/site-packages/altk/effcomm/analysis.py", line 28, in get_dataframe
            data=[tuple(lang.data[k] for k in columns) for lang in languages],
        File "/Users/nathanielimel/miniforge3/envs/modals-effcomm/lib/python3.10/site-packages/altk/effcomm/analysis.py", line 28, in <listcomp>
            data=[tuple(lang.data[k] for k in columns) for lang in languages],
        File "/Users/nathanielimel/miniforge3/envs/modals-effcomm/lib/python3.10/site-packages/altk/effcomm/analysis.py", line 28, in <genexpr>
            data=[tuple(lang.data[k] for k in columns) for lang in languages],
        KeyError: 'dlsav'

    Sure enough, "dlsav" is not one of the keys in the initialization the `data` attribute of a ModalLanguage in __init__. And so it is certainly possible for a language to not have been measured for the 'dlsav' property. My biggest worry, though, is why has this not been an issue before?
    
    Notably, it didn't happen for `data` or `pareto_data`, just `natural_data`. but presumably `data` contains `natural_data`. It is possible, though, that the languages were measured and then added to only artificial.yml, and so the natural.yml wasn't updated. 
        - Taking a look at the natural.yml file, I confirm this is the case. There are 5 natural languages, but only 4 contain the dlsav field.
        - javanese lacks dlsav.
        
        In fact (and this supports the idea it wasn't measured), javanese lacks all measurements.

            - javanese:
                data:
                Language: natural
                comm_cost: null
                complexity: null
                iff: null
                informativity: null
                name: javanese
                optimality: null
                sav: null
                simplicity: null

        What this means is that it is not sufficient to add the key for 'dlsav' in __init__, thought I now have. I need to ensure that the natural languages are saved after being measured. They are certainly measured.

    Something else interesting: only the 4 languages except javanese show up in artificial.yml. This could explain why I haven't seen the error before: maybe something specifically has happened to block javanese.

    I know for a fact that the following operation only takes a second or two, so it's probably the best solution for now, and it's readable.

        nat_langs = [lang for lang in langs if lang.is_natural()]
    
    Note that I could just as easily remove the natural languages using this logic.


7.6.22: full run of experiment with new hash function

    Interestingly, the correlation coefficients have each gone down by a few hundreths. 

        108701 total langs.

        Degree sav pearson correlations:
        simplicity: 0.4843976306855893
        informativity: 0.7577931130687842
        optimality: 0.651941190544032

    This isn't too surprising since the total langs has gone down significantly (by 70k). In fact, it's maybe surprising that it doesn't go down more.

    Also, the pattern on the plot is gorgeous, lol.

    But more importantly, the counts are more in the range that we presented at salt -- the legend shows them at 100 to 300. I'm not positive that those results we presented were wrong, since they were obtained by Shane -- but looking at the commit closest to it, it seemed to have the bad value_counts logic. But this together with the fact we weren't sampling nearly as much (literally less than 10% of this sample) could probably account for that.


    Upshot: I am feeling quite confident that these are the results we will present. I'm confident in how I'm measuring languages, and how I'm exploring the space. This commit has the message "full run with new language hash"

7.7.22: dlsav and natural statistics are exactly the same
status: resolved

    The two ttest results were exactly the same for natural and dlsav data. Looking into analysis, I had constructed the wrong dataframe. I had

        dlsav_data = natural_data[natural_data["dlsav"] == True]
    
    I changed to

        dlsav_data = data[data["dlsav"] == True]

7.7.22: TODO: add ProcessPool multiprocessing
status: not urgent

    The following is a list of things that should be parallelized but aren't yet:

        (estimated speedup: unknown)
        - Measuring each generation in altk evolutionary algorithm
        e.g.
            # Measure each generation
            for lang in languages:
                for m in self.objectives:
                    lang.data[m] = self.objectives[m](lang)

        (estimated speedup: it's complicated)

        - Each optimization corner of the evolutionary algorithm, 
        e.g.
            for direction in directions:
                x, y = directions[direction]
                ...
                optimizer.fit()

            Notes on the speedup estimate:

            For 200 generations and generation size of 2000, the second longest optimization corner is the 3rd, (comm_cost, simplicity -- upper left if we think of x=cost, y=comp), and takes 16:42. So for the main paper results, we'd probably get a speedup of about 16 minutes.

        (estimated speedup: minor; this currently takes less than 30 seconds)
        - Measuring all languages in altk tradeoff
        e.g.

            for lang in tqdm(languages):
                for prop in properties:
                    lang.data[prop] = properties[prop](lang)
                points.append((lang.data[x], lang.data[y]))

        (estimated speedup: probably same as above)
        - Measuring languages to obtain dataframe in altk analysis
        e.g.

            data = pd.DataFrame(
                data=[tuple(lang.data[k] for k in columns) for lang in languages],
                columns=columns,
            )

7.7.22: YAML File IO takes a long time saving and loading languages.
status: not urgent

    Here are some estimates:

        Loaded 108701 languages in 313.49 seconds
        Saved 108701 sampled languages in 153.51 seconds

    I'd be surprised if I could get substantial improvement. Future work can explore more heavy-duty file IO tools.

7.7.22: Results for pragmatic are suspiciously poor
status: resolved

    (for sav, on patas)
    
    Low optimality results for indicator_pragmatic:

        simplicity: 0.13611488839862035
        informativity: 0.40696227763875314
        optimality: 0.05992833718639482

    The only difference I can spot between indicator_literal and indicator_pragmatic is the sample_size paramter for random sampling, which is 2k vs 20k, respectively. However, since the evolutionary four-corner optimization technique applied, I'm puzzled why this still made such a difference.
    The number of discovered indicator_literal langs: 112991
    The number of discovered indicator_pragmatic langs: 45367

        Looking at the plot for indicator_pragmatic, I can immediately see one possible explanation: there are a _TON_ of somewhat optimal languages that are 0 degree SAV (large area, very large point sizes, etc). 
        Moreover, the majority, maybe 60% of the frontier consists of perfectly natural languages, especially as informativity increases. So essentially the indicator_pragmatic language space just consists of many perfectly unnatural languages that are fairly uninformative. This empirical fact is in line with how we intuitively reason about pragmatics -- when there's overlap, we have inference to exploit.


    Low optimality results for half_credit_pragmatic:

        simplicity: 0.08686105515347999
        informativity: 0.4707834625846252
        optimality: -0.24517766217213488

    The number of discovered half_credit_literal langs: 109422
    The number of discovered half_credit_pragmatic langs: 63422

        The plot has the same phenomenon for half_credit_pragmatic, though the effect feels less dramatic, probably because the entire plot is shifted over due to the nature of half_credit pragmatics (highest comm_cost is much lower).
    
    The major remaining issue is that it still doesn't seem right to say that optimality is _negatively_ correlated with sav. And likewise for the 0.05 correlation with optimality for indicator. 

    These results should be run again, and the random sample size should be increased to the level that half_credit_literal has.


    I ran all four configs again with a random sample size of 40k (took about 5h again). The results for literal speakers remain very positive (though different, see the random seed issue), but the results for pragmatic speakers are quite poor. In fact they are surprisingly poor given the plots: I would at least expect higher correlation with optimality.

        indicator pragmatic

        simplicity: 0.22544956948981418
        informativity: 0.38073428791953906
        optimality: 0.2591299914779223    

        plot: like 80% of optimal languages are perfectly natural. And there are many perfectly natural languages in the middle, e.g., not optimal. I observe a general trend towards both simplicity and informativeness; the coefficient for simplicity in particular I am surprised is fairly low. 
            - note compared to the entropy quantifiers results, these numbers aren't so different (optimality was .3).

        half_credit_pragmatic

        simplicity: 0.18032611628317569
        informativity: 0.43323971354696705
        optimality: -0.11936231642150943

        plot: more than half the optimal languages are perfectly natural. But there are many perfectly natural languages that are in the middle, e.g., not optimal. However, I do observe a general trend towards informativeness, which seems to be captured in the coefficient. But I'm still surprised about optimality.

    Shane's first pass:

        Just as a reminder: pragmatic reasoning will make comm cost go down without affecting complexity at all, so that should have the effect of lowering the correlation w/ complexity I think.

        Where exactly are complexity and cost being measured? In the following sense: cost is a property of sender/receiver, i.e. 1-I(S, R).  But in the way we currently have a separation btn langs and S/R, complexity should be a property of the lang, that is uneffected by which S/R are using it?

        Me: Yeah, complexity is fully independent.

        But then why does the correlation change so dramatically btn the literal and pragmatic case? That's the part I'm missing.  Is it just sample size, or something else?

        Me: There is a difference in sample size, but its not dramatic (I tried to make all my config files minimally diff and so they have the same random sampling and evol alg params). For indicator prag we have 75k, for half credit prag 85k.
        Ill need to check out complexity. One strategy could be to just load the literal generated languages and measure them with pragmatic to see if complexities change.

        Test 0: load both experiment's languages and compare complexity of counterparts.

            I wrote a script to do this. These were results:
            
                Loaded 94914 languages in 258.71 seconds
                Loaded 85953 languages in 232.62 seconds
                found 34693 matching literal and pragmatic languages.
                found 0 discrepancies in complexity.

            What this means is that there were actually at least 50k languages that were in fact disjoint. This is initially somewhat surprising to me, because I had been under the impression that experiments, even with different measurement functions (speaker type, utility function) basically explore the same _space_, the same underlying pool of languages. But perhaps something about the measures encourages different areas of the space to be explore?

            I'll need to think about this: in particular, assume the differences in complexity really come down to differences in sampling. That is, even though the two experiments explore roughly the same _size_ of languages overall, the set that they explore is mostly non overlapping.

            Reasons I can think to explain this:
                - the evolutionary algorithm is encouraged to explore different languages, based on the measures. (but in what exact way?)


            - Followup / counterpoint to the script: I should have first looked just at the _overlapping_ languages using simple set() logic, since that's essentially the logic I wanted to use for building the comparison script. So I loaded both lists of languages, and took their intersection.

                >>> result_literal = load_languages("/Users/nathanielimel/clms/projects/modals-effcomm/outputs/half_credit_literal/languages/artificial.yml")
                Loaded 94914 languages in 261.41 seconds
                >>> result_pragmatic = load_languages("/Users/nathanielimel/clms/projects/modals-effcomm/outputs/half_credit_pragmatic/languages/artificial.yml")
                Loaded 85953 languages in 242.96 seconds
                >>> literal_languages = result_literal["languages"]
                >>> pragmatic_languages = result_literal["languages"]
                >>> pool_intersection = set(literal_languages).intersection(set(pragmatic_languages))
                >>> len(pool_intersection)
                94914

        This is surprising. How can I take a set B that is strictly smaller than A, and have A intersect B be the same size as A? The intersection must be less than or equal to the number of elements in the smallest argument. So I checked with dominant and natural languages, and found (as expected) there was 0 in that intersection, which means the hash function used in __contains__ is producing the expected result for this latter case.

        Okay, so I tried to reproduce the results and did not get this again. In particular,

            >>> result_literal = load_languages("/Users/nathanielimel/clms/projects/modals-effcomm/outputs/half_credit_literal/languages/artificial.yml")
            Loaded 94914 languages in 261.74 seconds
            >>> result_pragmatic = load_languages("/Users/nathanielimel/clms/projects/modals-effcomm/outputs/half_credit_pragmatic/languages/artificial.yml")
            Loaded 85953 languages in 234.39 seconds
            >>> pragmatic_languages = result_pragmatic["languages"]
            >>> len(pragmatic_languages)
            85953
            >>> literal_languages = result_literal["languages"]
            >>> len(literal_languages)
            94914
            >>> len(set(literal_languages))
            94914
            >>> len(set(pragmatic_languages))
            85953
            >>> overlapping_languages = set(literal_languages).intersection(set(pragmatic_languages))
            >>> len(overlapping_languages)
            34693

        This number is the one I expected. So there must be a sampling difference. And this sampling difference must be enough that the languages that _are_ explored in the pragmatic are such that the SAV ones aren't related to the optimal ones.

        Next, I want to proceed with Test 1.

        Test 1: load languages from pragmatic experiment and measure them in literal experiment (measure_tradeoff and analyze)

            - The simplest way to proceed is just to change the file paths manually in the config file.

        These were the original half_credit_literal result:

            simplicity: 0.46805778888062943
            informativity: 0.7724286934553124
            optimality: 0.6457743360494049

        And here are the results when measuring the languages produced in the pragmatic version of the experiment:          

            Degree sav pearson correlations:
            simplicity: 0.18032611628319356
            informativity: 0.5361134836542771
            optimality: 0.27951462523025844

            MEANS
                            simplicity  informativity  optimality
            name                                                   
            natural_means       0.795000       0.746296    0.951134
            dlsav_means         0.679170       0.611892    0.839120
            population_means    0.541366       0.532717    0.729446

            TTEST STATS
            natural languages against population
                            simplicity  informativity  optimality
            stat                                                    
            t-statistic         12.681693       3.019436    8.392832
            Two-sided p-value    0.000223       0.039185    0.001103

            dlsav languages against population
                            simplicity  informativity  optimality
            stat                                                    
            t-statistic         61.403119   3.016456e+01   70.137804
            Two-sided p-value    0.000000  3.766663e-177    0.000000

        It is a good thing that simplicity correlation is exactly as low, and that the informativity is slightly lower (because the pragmatic languages don't need to be as literally informative). Because the quantitative results are what they should be if only the input pool of languages differed, this makes me confident that the difference of interest between the literal and pragmatic results is not our measures, but the fact that the two experiments end up exploring a different, but similarly sized, pool of languages. And this invites us to think about whether, and how to better sample the space.

        Test 2: load languages from literal experiment and measure them in pragmatic experiment.

            - proceeding by changing the file paths manually in the config file.


        the results:

            Degree sav pearson correlations:
            simplicity: 0.46805778888050487
            informativity: 0.0651700139555651
            optimality: 0.39889141958071855

            MEANS
                            simplicity  informativity  optimality
            name                                                   
            natural_means       0.806250       0.965972    0.862867
            dlsav_means         0.646948       1.153990    0.735540
            population_means    0.471054       0.930971    0.628848

            TTEST STATS
            natural languages against population
                            simplicity  informativity  optimality
            stat                                                    
            t-statistic         15.701675       0.198313    3.141778
            Two-sided p-value    0.000561       0.855478    0.051592

            dlsav languages against population
                            simplicity  informativity     optimality
            stat                                                       
            t-statistic         88.205789       42.43659   3.776001e+01
            Two-sided p-value    0.000000        0.00000  5.896111e-271


        Notice how different informativity is now. Compare this to the most recent run of full experiment with pragmatic settings, where simplicity has less than half the correlation strength, and informativity is as high as simplicity is now:

            simplicity: 0.18032611628317569
            informativity: 0.43323971354696705
            optimality: -0.11936231642150943

        The plot is informative. Compared to the plot for the full experiment for the pragmatic setting, there are far more languages at the bottom of the plot, and many of them are fully natural. (It does, as it should, look as if the literal experiment results were squashed). And there are far LESS languages in the upper left; in the original full pragmatic experiment, the densest area was in the upper left corner, and full of fully unnatural languages.

        So I'm confident that what we have is a sampling issue: the language pool from each experiment is not the same. This raises a few questions: What is the correct pool for each experiment? Should they all be the same, maximal pool? And how do we know if we've found it?

        I'm worried about whether we can improve sampling in a principled way for the pragmatic measure: why is the resulting sample, of optimizing all four corners of the space, giving such different result than if we merely change the informativity measure to give less reward (e.g. be more aggressive)

7.16.22: Abstracted meaning point into a class instead of just storing strings as the elements of meanings.
status: resolved

    Ran through dev after fixing a few easy bugs. If issues come up, the commit hash for this update is: 
        4e2b71d03368c0fb4832e9f04df237a1b03aeeb2
    with commit message:
        updated codebase with ModalMeaningPoint logic


7.21.22: Updated altk for a refactor with signaling. Moved some functions from informativity to a util module. Also, a major change is that  now communicative_success takes a utility function, not a utility matrix. This will require small fixes here.
status: resolved

11.19.22: Finally refactoring the ugly `is_natural` code in `modal_language`, which also means that function needs to be removed in altk.
status:

Although the actually bug was motivated by losing natural languages after measure_tradeoff, it was nice to clean this up. The move was to go with a property, which updates the data dict of a language every time its set.

The cause of losing natural languages during measure tradeoff was due to the list(set()) operation of uniquing languages. 

However, I would have thought that two languages would have been considered unique even if they have all the same expressions, but one is natural and the other isn't. And if they're unique, then they won't get dropped by the set() operation.

Interestingly, it could be a problem _within_ the natural languages, not between the naturals and artificials:

        >>> len(nat_langs)
        18
        >>> len(set(nat_langs))
        16

I took a look at the languages which are marked equal:

        >>> for eq in eqs:
        ...     print(eq[0].data["name"], eq[1].data["name"])
        ... 
        Donmari Goemai
        Goemai Donmari
        Mian Vaeakau-Taumako
        Vaeakau-Taumako Mian

Looking at the languages, we see that Donmari and Goemai have the same modals: weak+deontic and strong+deontic. Meanwhile, we see that Mian and Vaeakau-Taumako have the same modals: strong+deontic and strong+deontic.


Looks like I'll need to edit the hashing or equality function, and maybe encode that natural languages are always unique or something. A complex conditional hashing looks gerrymandered, but I think it describes exactly what we want, so it'll end up being the best move. So the new hashing function looks like:


        if self.natural:
            expressions_hash = hash(tuple(sorted([hash(e) for e in self.expressions])))
        else:
            # hash a tuple of the sorted list of LoT strings in a language
            expressions_hash = hash(
                tuple(sorted([e.lot_expression for e in self.expressions]))
            )
        return expressions_hash


    And the resulting set operation on natural languages looks like:

        >>> len(nat_langs)
        18
        >>> len(set(nat_langs))
        18

    While for artificials, we have

        >>> len(dom_langs)
        12
        >>> len(set(dom_langs))
        12    
